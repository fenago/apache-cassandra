{
  "paragraphs": [
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1591531690407_-746183399",
      "id": "paragraph_1591531690407_-746183399",
      "dateCreated": "2020-06-07T12:08:10+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:1552",
      "text": "%md\n\n\n\nLab 7. Deployment and Monitoring\n---------------------------------------------\n\n**Note:** We will run this example on single node like previous labs. This lab contains information regarding casssandra on multiple nodes instead.\n\nWe have explored the development of the Stock Screener Application in\nprevious labs; it is now time to consider how to deploy it in the\nproduction environment. In this lab, we will discuss the most\nimportant aspects of deploying a Cassandra database in production. These\naspects include the selection of an appropriate combination of\nreplication strategy, snitch, and replication factor to make up a\nfault-tolerant, highly available cluster. Then we will demonstrate the\nprocedure to migrate our Cassandra development database of the Stock\nScreener Application to a production database. However, cluster\nmaintenance is beyond the scope of this course.\n\nMoreover, a live production system that continuously operates certainly\nrequires monitoring of its health status. We will cover the basic tools\nand techniques of monitoring a Cassandra cluster, including the nodetool\nutility, JMX and MBeans, and system log.\n\nFinally, we will explore ways of boosting the performance of Cassandra\nother than using the defaults. Actually, performance tuning can be made\nat several levels, from the lowest hardware and system configuration to\nthe highest application coding techniques. We will focus on the **Java\nVirtual Machine** (**JVM**)\nlevel, because Cassandra heavily relies on its underlying performance.\nIn addition, we will touch on how to tune caches for a table.\n\n\nReplication strategies\n----------------------------------------\n\n\n\nThis section is about the data replication\nconfiguration of a Cassandra cluster. It will cover replication\nstrategies, snitches, and the configuration of the cluster for the Stock\nScreener Application.\n\n\n\n### Data replication\n\n\nCassandra, by design, can work in a huge cluster\nacross multiple data centers all over the globe. In\nsuch a distributed environment, network bandwidth and latency must be\ncritically considered in the architecture, and careful planning in\nadvance is required, otherwise it would lead to catastrophic\nconsequences. The most obvious issue is the time clock\nsynchronization---the genuine means of resolving transaction conflicts\nthat can threaten data integrity in the whole cluster. Cassandra\nthe underlying\noperating system platform to provide the time clock synchronization\nservice. Furthermore, a node is highly likely to fail at some time and\nthe cluster must be resilient to this typical node failure. These issues\nhave to be thoroughly considered at the architecture level.\n\nCassandra adopts data replication to tackle these issues, based on the\nidea of using space in exchange of time. It simply consumes more storage\nspace to make data replicas so as to minimize the complexities in\nresolving the previously mentioned issues in a cluster.\n\nData replication is configured by the so-called replication factor in a\n **keyspace**. The replication factor\nrefers to the total number of copies of each row across the cluster. So\na replication factor of `1` (as seen in the examples in\nprevious labs) means that there is only one copy of each row on one\nnode. For a replication factor of `2` , two copies of each row\nare on two different nodes. Typically, a replication factor of\n`3` is sufficient in most production scenarios.\n\nAll data replicas are equally important. There are neither master nor\nslave replicas. So data replication does not have scalability issues.\nThe replication factor can be increased as more nodes are added.\nHowever, the replication factor should not be set to exceed the number\nof nodes in the cluster.\n\nAnother unique feature of Cassandra is its awareness of the physical\nlocation of nodes in a cluster and their proximity to each other.\nCassandra can be configured to know the layout of the data centers and\nracks by a correct IP address assignment scheme. This setting is known\nas replication strategy and Cassandra provides two choices for us:\n`SimpleStrategy` and `NetworkTopologyStrategy`.\n\n\n### SimpleStrategy\n\n\n`SimpleStrategy` is  used on a single\nmachine or on a cluster in a single data center. It\nplaces the first replica on a node determined by the partitioner, and\nthen the additional replicas are placed on the next nodes in a clockwise\nfashion without considering the data center and rack locations. Even\nthough this is the default replication strategy when creating a\nkeyspace, if we ever intend to have more than one data center, we should\nuse `NetworkTopologyStrategy` instead.\n\n\n### NetworkTopologyStrategy\n\n\n`NetworkTopologyStrategy` becomes\naware of the locations of data centers and racks by\nunderstanding the IP addresses of the nodes in the cluster. It places\nreplicas in the same data center by the clockwise mechanism until the\nfirst node in another rack is reached. It attempts to place replicas on\ndifferent racks because the nodes in the same rack often fail at the\nsame time due to power, network issues, air conditioning, and so on.\n\nAs mentioned, Cassandra knows the physical location from the IP\naddresses of the nodes. The mapping of the IP addresses to the data\ncenters and racks is referred to as a \n**snitch**. Simply put, a snitch determines which data\ncenters and racks the nodes belong to. It optimizes read operations by\nproviding information about the network topology to Cassandra such that\nread requests can be routed efficiently. It also affects how replicas\ncan be distributed in consideration of the physical location of data\ncenters and racks.\n\nThere are many types of snitches available for different scenarios and\neach comes with its pros and cons. They are briefly described as\nfollows:\n\n\n-   `SimpleSnitch`: This is used for\n    single data center deployments only\n\n-   `DynamicSnitch`: This monitors the\n    performance of read operations from different\n    replicas, and chooses the best replica based on historical\n    performance\n\n-   `RackInferringSnitch`: This \n    determines the location of the nodes\n    by data center and rack corresponding to the IP addresses\n\n-   `PropertyFileSntich`: This determines\n    the locations of the nodes by data center and\n    rack\n\n-   `GossipingPropertyFileSnitch`: This \n    automatically updates all nodes using gossip\n    when adding new nodes\n\n-   `EC2Snitch`: This  is used \n    with Amazon EC2 in a single region\n\n-   `EC2MultiRegionSnitch`: This is used\n    with Amazon EC2 in multiple regions\n\n-   `GoogleCloudSnitch`: This  is used\n    with Google Cloud Platform across one or more\n    regions\n\n-   `CloudstackSnitch`: This is  used for\n    Apache Cloudstack environments\n\n### Note\n\n**Snitch Architecture**\n\nFor more detailed information, please refer to the documentation made by\nDataStax at\n<http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html>.\n\nThe following figure illustrates an example of a cluster of eight nodes\nin four racks across two data centers using\n`RackInferringSnitch` and a replication factor of three per\ndata center:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_01.jpg)\n\n\n### Tip\n\nAll nodes in the cluster must use the same snitch setting.\n\nLet us look at the IP address assignment in **Data Center 1**\nfirst. The IP addresses are grouped and assigned in a top-down fashion.\nAll the nodes in **Data Center 1** are in the same\n**123.1.0.0** subnet. For those nodes in **Rack\n1**, they are in the same **123.1.1.0** subnet.\nHence, **Node 1** in **Rack 1** is assigned an IP\naddress of **123.1.1.1** and **Node 2** in **Rack\n1** is **123.1.1.2**. The same rule applies to\n**Rack 2** such that the IP addresses of **Node\n1** and **Node 2** in **Rack 2** are\n**123.1.2.1** and **123.1.2.2**, respectively. For\n**Data Center 2**, we just change the subnet of the data\ncenter to **123.2.0.0** and the racks and nodes in **Data\nCenter 2** are then changed similarly.\n\nThe `RackInferringSnitch` deserves a more\ndetailed explanation. It assumes that the network topology is known by\nproperly assigned IP addresses based on the following rule:\n\n*IP address = \\<arbitrary octet\\>.\\<data center octet\\>.\\<rack\noctet\\>.\\<node octet\\>*\n\nThe formula for IP address assignment is shown in the previous\nparagraph. With this very structured assignment of IP addresses,\nCassandra can understand the physical location of all nodes in the\ncluster.\n\nAnother thing that we need to understand is the replication factor of\nthe three replicas that are shown in the previous figure. For a cluster\nwith `NetworkToplogyStrategy` , the replication factor is set\non a per data center basis. So in our example, three replicas are placed\nin **Data Center 1** as illustrated by the dotted arrows in\nthe previous diagram. **Data Center 2** is another data\ncenter that must have three replicas. Hence, there are six replicas in\ntotal across the cluster.\n\nWe will not go through every combination of the replication factor,\nsnitch and replication strategy here, but we should now understand the\nfoundation of how Cassandra makes use of them to flexibly deal with\ndifferent cluster scenarios in real-life production.\n\n\n### Setting up the cluster for Stock Screener Application\n\n\nLet us return to the Stock Screener Application.\nThe cluster it runs on ,\n*Enhancing a Version*, is a single-node cluster. In\nthis section, we will set up a cluster of two nodes\nthat can be used in small-scale production. We will also migrate the\nexisting data in the development database to the new fresh production\ncluster. It should be noted that for quorum reads/writes, it's usually\nbest practice to use an odd number of nodes.\n\n\n\n#### System and network configuration\n\n\nThe steps of installation and setup of the\noperating system and network configuration are assumed to be done.\nMoreover, both nodes should have Cassandra freshly installed. The system\nconfiguration of the two nodes is identical and shown as follows:\n\n\n-   OS: Ubuntu 12.04 LTS 64-bit\n\n-   Processor: Intel Core i7-4771 CPU \\@3.50GHz x 2\n\n-   Memory: 2 GB\n\n-   Disk: 20 GB\n\n\n\n#### Global settings\n\n\nThe  **Test\nCluster**, in which both the\n**ubtc01** and **ubtc02** nodes are in the same rack, `RACK1` , and in the\nsame data center, `NY1`. The logical architecture of the\ncluster to be set up is depicted in the following diagram:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_02.jpg)\n\nIn order to configure a Cassandra cluster, we need to modify a few\nproperties in the main configuration file, `cassandra.yaml` ,\nfor Cassandra. Depending on the installation method of Cassandra,\n`cassandra.yaml` is located in different directories:\n\n\n-   Package installation: `/etc/cassandra/`\n\n-   Tarball installation: `<install_location>/conf/`\n:::\n\nThe first thing to do is to set the properties in\n`cassandra.yaml` for each node. As the system configuration of\nboth nodes is the same, the following modification on\n`cassandra.yaml` settings is identical to them:\n\n\n``` {.programlisting .language-markup}\n-seeds: ubtc01\nlisten_address:\nrpc_address: 0.0.0.0\nendpoint_snitch: GossipingPropertyFileSnitch\nauto_bootstrap: false\n```\n\nThe reason for using `GossipingPropertyFileSnitch` is that we\nwant the Cassandra cluster to automatically update all nodes with the\ngossip protocol when adding a new node.\n\nApart from `cassandra.yaml` , we also need to modify the data\ncenter and rack properties in `cassandra-rackdc.properties` in\nthe same location as `cassandra.yaml`. In our case, the data\ncenter is `NY1` and the rack is `RACK1` , as shown in\nthe following code:\n\n\n``` {.programlisting .language-markup}\ndc=NY1\nrack=RACK1\n```\n\n\n#### Configuration procedure\n\n\nThe configuration procedure of the cluster (refer\nto the following bash shell scripts: `setup_ubtc01.sh` and\n`setup_ubtc02.sh`) is enumerated as follows:\n\n\n1.  Stop Cassandra service:\n\n\n    ``` {.programlisting .language-markup}\n    ubtc01:~$ sudo service cassandra stop\n    ubtc02:~$ sudo service cassandra stop\n    ```\n    :::\n\n2.  Remove the system keyspace:\n\n\n    ``` {.programlisting .language-markup}\n    ubtc01:~$ sudo rm -rf /var/lib/cassandra/data/system/*\n    ubtc02:~$ sudo rm -rf /var/lib/cassandra/data/system/*\n    ```\n    :::\n\n3.  Modify `cassandra.yaml` and\n    `cassandra-rackdc.properties` in both nodes based on the\n    global settings as specified in the previous section\n\n4.  Start the seed node `ubtc01` first:\n\n\n    ``` {.programlisting .language-markup}\n    ubtc01:~$ sudo service cassandra start\n    ```\n    :::\n\n5.  Then start `ubtc02`:\n\n\n    ``` {.programlisting .language-markup}\n    ubtc02:~$ sudo service cassandra start\n    ```\n    :::\n\n6.  Wait for a minute and check if `ubtc01` and\n    `ubtc02` are both up and running:\n\n\n    ``` {.programlisting .language-markup}\n    ubtc01:~$ nodetool status\n    ```\n    :::\n:::\n\nA successful result of setting up the cluster\nshould resemble something similar to the following screenshot, showing\nthat both nodes are up and running:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_03.jpg)\n\n\n#### Legacy data migration procedure\n\n\nWe now have the cluster ready but it is empty. We\ncan simply rerun the Stock Screener Application to download and fill in\nthe production database again. Alternatively, we can migrate the\nhistorical prices collected in the development single-node cluster to\nthis production cluster. In the case of the latter approach, the\nfollowing procedure can help us ease the  data\nmigration task:\n\n\n1.  Take a snapshot of the `packcdma` keyspace in the\n    development database (ubuntu is the hostname of the development\n    machine):\n\n\n    ``` {.programlisting .language-markup}\n    ubuntu:~$ nodetool snapshot fenagocdma\n    ```\n    :::\n\n2.  Record the snapshot directory, in this example,\n    **1412082842986**\n\n3.  To play it safe, copy all SSTables under the snapshot directory to a\n    temporary location, say `~/temp/`:\n\n\n    ``` {.programlisting .language-markup}\n    ubuntu:~$ mkdir ~/temp/\n    ubuntu:~$ mkdir ~/temp/fenagocdma/\n    ubuntu:~$ mkdir ~/temp/fenagocdma/alert_by_date/\n    ubuntu:~$ mkdir ~/temp/fenagocdma/alertlist/\n    ubuntu:~$ mkdir ~/temp/fenagocdma/quote/\n    ubuntu:~$ mkdir ~/temp/fenagocdma/watchlist/\n    ubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/alert_by_date/snapshots/1412082842986/* ~/temp/fenagocdma/alert_by_date/\n    ubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/alertlist/snapshots/1412082842986/* ~/temp/fenagocdma/alertlist/\n    ubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/quote/snapshots/1412082842986/* ~/temp/fenagocdma/quote/\n    ubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/watchlist/snapshots/1412082842986/* ~/temp/fenagocdma/watchlist/\n    ```\n    :::\n\n4.  Open cqlsh to connect to `ubtc01` and create a keyspace\n    with the appropriate replication strategy in the production cluster:\n\n\n    ``` {.programlisting .language-markup}\n    ubuntu:~$ cqlsh ubtc01\n    cqlsh> CREATE KEYSPACE fenagocdma WITH replication = {'class': 'NetworkTopologyStrategy',  'NY1': '2'};\n    ```\n    :::\n\n5.  Create the `alert_by_date` , `alertlist` ,\n    `quote` , and `watchlist` tables:\n\n\n    ``` {.programlisting .language-markup}\n    cqlsh> CREATE TABLE fenagocdma.alert_by_date (\n      price_time timestamp,\n      symbol varchar,\n      signal_price float,\n      stock_name varchar,\n      PRIMARY KEY (price_time, symbol));\n    cqlsh> CREATE TABLE fenagocdma.alertlist (\n      symbol varchar,\n      price_time timestamp,\n      signal_price float,\n      stock_name varchar,\n      PRIMARY KEY (symbol, price_time));\n    cqlsh> CREATE TABLE fenagocdma.quote (\n      symbol varchar,\n      price_time timestamp,\n      close_price float,\n      high_price float,\n      low_price float,\n      open_price float,\n      stock_name varchar,\n      volume double,\n      PRIMARY KEY (symbol, price_time));\n    cqlsh> CREATE TABLE fenagocdma.watchlist (\n      watch_list_code varchar,\n      symbol varchar,\n      PRIMARY KEY (watch_list_code, symbol));\n    ```\n    :::\n\n6.  Load the SSTables back to the production\n    cluster using the `sstableloader` utility:\n\n\n    ``` {.programlisting .language-markup}\n    ubuntu:~$ cd ~/temp\n    ubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/alert_by_date\n    ubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/alertlist\n    ubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/quote\n    ubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/watchlist\n    ```\n    :::\n\n7.  Check the legacy data in the production database on\n    `ubtc02`:\n\n\n    ``` {.programlisting .language-markup}\n    cqlsh> select * from fenagocdma.alert_by_date;\n    cqlsh> select * from fenagocdma.alertlist;\n    cqlsh> select * from fenagocdma.quote;\n    cqlsh> select * from fenagocdma.watchlist;\n    ```\n    :::\n:::\n\nAlthough the previous steps look complicated, it is\nnot difficult to understand what they want to achieve. It should be\nnoted that we have set the replication factor per data center as\n`2` to provide data redundancy on both nodes, as shown in the\n`CREATE KEYSPACE` statement. The replication factor can be\nchanged in future if needed.\n\n\n#### Deploying the Stock Screener Application\n\n\nAs we have set up the production cluster and moved\nthe legacy data into it, it is time to deploy the Stock Screener\nApplication. The only thing needed to modify is the code to establish\nCassandra connection to the production cluster. This is very easy to do\nwith Python. The code in `lab06_006.py` is modified to\nwork with the production cluster as `lab07_001.py`. A new\ntest case named `testcase003()` is created to replace\n`testcase002()`. To save pages, the complete source code of\n`lab07_001.py` is not shown here; only the\n`testcase003()` function is depicted as follows:\n\n\n``` {.programlisting .language-markup}\n# -*- coding: utf-8 -*-\n# program: lab07_001.py\n\n# other functions are not shown for brevity\n\ndef testcase003():\n    ## create Cassandra instance with multiple nodes\n    cluster = Cluster(['ubtc01', 'ubtc02'])\n    \n    ## establish Cassandra connection, using local default\n    session = cluster.connect('fenagocdma')\n    \n    start_date = datetime.datetime(2012, 6, 28)\n    end_date = datetime.datetime(2013, 9, 28)\n    \n    ## load the watch list\n    stocks_watched = load_watchlist(session, \"WS01\")\n    \n    for symbol in stocks_watched:\n        ## retrieve data\n        data = retrieve_data(session, symbol, start_date, end_date)\n        \n        ## compute 10-Day SMA\n        data = sma(data, 10)\n        \n        ## generate the buy-and-hold signals\n        alerts = signal_close_higher_than_sma10(data)\n        \n        ## save the alert list\n        for index, r in alerts.iterrows():\n            insert_alert(session, symbol, index, \\\n                         Decimal(r['close_price']), \\\n                         r['stock_name'])\n    \n    ## close Cassandra connection\n    cluster.shutdown()\n\ntestcase003()\n```\n\nThe cluster connection code right at the beginning\nof the `testcase003()` function is passed with an array of the\nnodes to be connected (`ubtc01` and `ubtc02`). Here\nwe adopted the default `RoundRobinPolicy` as the connection\nload balancing policy. It is used to decide how to distribute requests\namong all possible coordinator nodes in the cluster. There are many\nother options which are described in the driver API documentation.\n\n\n### Note\n\n**Cassandra Driver 2.1 Documentation**\n\nFor the complete API documentation of the Python\nDriver 2.1 for Apache Cassandra, you can refer to\n<http://datastax.github.io/python-driver/api/index.html>.\n\n\nMonitoring\n----------------------------\n\n\n\nAs the  application system goes live, we need to\nmonitor its health day-by-day. Cassandra provides a number of tools for\nthis purpose. We will introduce some of them with pragmatic\nrecommendations. It is remarkable that each operating system also\nprovides a bunch of tools and utilities for\nmonitoring, for example, `top` , `df` , `du`\non Linux and Task Manager on Windows. However, they are beyond the scope\nof this course.\n\n\n\n### Nodetool\n\n\nThe  utility should\nnot be new to us. It is a command-line interface used to monitor\nCassandra and perform routine database operations. It includes the most\nimportant metrics for tables, server, and compaction statistics, and\nother useful commands for administration.\n\nHere are the most commonly used `nodetool` options:\n\n\n-   `status`: This  provides a concise\n    summary of the cluster, such as the state, load, and IDs\n\n-   `netstats`: This gives the network\n    information for a node, focusing on read repair operations\n\n-   `info`: This gives valuable node\n    information including token, on disk load, uptime, Java heap memory\n    usage, key cache, and row cache\n\n-   `tpstats`: This provides statistics\n    about the number of active, pending, and completed tasks for each\n    stage of Cassandra operations by thread pool\n\n-   `cfstats`: This gets the statistics\n    about one or more tables, such as read-and-write counts and\n    latencies, metrics about SSTable, memtable, bloom filter, and\n    compaction.\n\n### Note\n\nA detailed documentation of nodetool can be\nreferred to at\n<http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html>.\n\n\n### JMX and MBeans\n\n\nCassandra written in the\nJava language and so it natively supports **Java\nManagement Extensions** (**JMX**). We may use\nJConsole, a JMX-compliant tool, to monitor\nCassandra.\n\n\n### Note\n\n**JConsole**\n\nJConsole is included with Sun JDK 5.0 and higher\nversions. However, it consumes a significant amount of system resources.\nIt is recommended that you run it on a remote machine rather than on the\nsame host as a Cassandra node.\n\nWe can launch JConsole by typing\n`jconsole` in a terminal. Assuming that we want to monitor the\nlocal node, when the **New Connection** dialog box pops up,\nwe type `localhost:7199` (`7199` is the port number\nof JMX) in the **Remote Process** textbox, as depicted in the\nfollowing screenshot:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_04.jpg)\n\nAfter having connected to the local Cassandra instance, we will see a\nwell-organized GUI showing six separate tabs placed horizontally on the\ntop, as seen in the following screenshot:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_05.jpg)\n\nThe tabs of the GUI are explained as follows:\n\n\n-   **Overview**: This displays overview information about\n    the JVM and monitored values\n\n-   **Memory**: This displays information about heap and\n    non-heap memory usage and garbage collection metrics\n\n-   **Threads**: This displays information about thread use\n\n-   **Classes**: This displays information about class\n    loading\n\n-   **VM Summary**: This displays information about the JVM\n\n-   **MBeans**: This displays information about specific\n    Cassandra metrics and operations\n:::\n\nFurthermore, Cassandra provides five MBeans for\nJConsole. They are briefly introduced as follows:\n\n\n-   `org.apache.cassandra.db`: This\n    includes caching, table metrics, and compaction\n\n-   `org.apache.cassandra.internal`: These\n    are internal server operations such as gossip and hinted\n    handoff\n\n-   `org.apache.cassandra.metrics`: These\n    are various metrics of the Cassandra instance such as cache and\n    compaction\n\n-   `org.apache.cassandra.net`: This has\n    Inter-node communication including FailureDetector, MessagingService\n    and StreamingService\n\n-   `org.apache.cassandra.request`: These\n    include tasks related to read, write, and replication operations\n\n### Note\n\n**MBeans**\n\nAn **Managed Bean** (**MBean**) is a Java object that represents a manageable resource such as\nan application, a service, a component, or a device running in the JVM.\nIt can be used to collect statistics on concerns such as performance,\nresource usage, or problems, for getting and setting application\nconfigurations or properties, and notifying events like faults or state\nchanges.\n\n\n### The system log\n\n\nThe most  yet the\nmost powerful, monitoring tool is Cassandra's system log. The default\nlocation of the system log is named `system.log` under\n`/var/log/cassandra/`. It is simply a text file and can be\nviewed or edited by any text editor. The following screenshot shows an\nextract of `system.log`:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_06.jpg)\n\nThis piece log looks long\nand weird. However, if you are a Java developer and you are familiar\nwith the standard log library, Log4j, it is pretty straightforward. The\nbeauty of Log4j is the provision of different log levels for us to\ncontrol the granularity of the log statements to be recorded in\n`system.log`. As shown in the previous figure, the first word\nof each line is `INFO` , meaning that the log statement is a\npiece of information. Other log level choices include `FATAL` ,\n`ERROR` , `WARN` , `DEBUG` , and\n`TRACE` , from the least verbose to the most verbose.\n\nThe system log is very valuable in troubleshooting problems as well. We\nmay increase the log level to `DEBUG` or `TRACE` for\ntroubleshooting. However, running a production Cassandra cluster in the\n`DEBUG` or `TRACE` mode will degrade its performance\nsignificantly. We must use them with great care.\n\nWe can change the standard log level in Cassandra by adjusting the\n`log4j.rootLogger` property in\n`log4j-server.properties` in the Cassandra configuration\ndirectory. The following screenshot shows the content of\n`log4j-server.properties` in ubtc02:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_07.jpg)\n\nIt is important to mention that\n`system.log` and `log4j-server.properties` are only\nresponsible for a single node. For a cluster of two nodes, we\nwill have two `system.log` and two\n`log4j-server.properties` on the respective nodes.\n\n\n\nPerformance tuning\n------------------------------------\n\n\n\nPerformance tuning is a large and complex topic\nthat in itself can be a whole course. We can only scratch the surface of\nit in this short section. Similar to monitoring in the last section,\noperating system-specific performance tuning techniques are beyond the\nscope of this course.\n\n\n\n### Java virtual machine\n\n\nBased on the information given by the monitoring\ntools and the system log, we can discover\nopportunities for performance tuning. The first things we usually watch\nare the Java heap memory and garbage collection. JVM's configuration\nsettings are controlled in the environment settings\nfile for Cassandra, `cassandra-env.sh` , located in\n`/etc/cassandra/`. An example is shown in the following\nscreenshot:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_08.jpg)\n\nBasically, it already has the boilerplate options\ncalculated to be optimized for the host system. It is also accompanied\nwith explanation for us to tweak specific JVM parameters and the startup\noptions of a Cassandra instance when we experience real issues;\notherwise, these boilerplate options should not be altered.\n\n\n### Note\n\nA detailed documentation on how to tune JVM for\nCassandra can be found at\n<http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html>.\n\n\n### Caching\n\n\nAnother we should pay\nattention to is caching. Cassandra includes integrated caching and\ndistributes cache data around the cluster. For a cache specific to a\ntable, we will focus on the partition key cache and the row cache.\n\n\n\n#### Partition key cache\n\n\nThe partition key cache, or key cache for short, is a cache of the partition index for a\ntable. Using the key cache saves processor time and memory. However,\nenabling just the key cache makes the disk activity actually read the\nrequested data rows.\n\n\n#### Row cache\n\n\nThe  is similar to a\ntraditional cache. When a row is accessed, the entire row is pulled into\nmemory, merging from multiple SSTables when required, and cached. This\nprevents Cassandra from retrieving that row using disk I/O again, which\ncan tremendously improve read performance.\n\nWhen both row cache and partition key cache are configured, the row\ncache returns results whenever possible. In the event of a row cache\nmiss, the partition key cache might still provide a hit that makes the\ndisk seek much more efficient.\n\nHowever, there is one caveat. Cassandra caches all the rows of a\npartition when reading that partition. So if the partition is large or\nonly a small portion of the partition is read every time, the row cache\nmight not be beneficial. It is very easy to be misused and consequently\nthe JVM will be exhausted, causing Cassandra to fail. That is why the\nrow cache is disabled by default.\n\n\n### Note\n\nWe usually enable either the key or row cache for a table, not both at\nthe same time.\n\n\n#### Monitoring cache\n\n\nEither the `nodetool info` command or JMX MBeans can provide assistance in monitoring\ncache. We should make changes to cache options in small, incremental\nadjustments, and then monitor the effects of each change using the\nnodetool utility. The last two lines of output of the\n`nodetool info` command, as seen in the following figure,\ncontain the `Row Cache` and `Key Cache` metrics of\n`ubtc02`:\n\n\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_09.jpg)\n\nIn the of high memory\nconsumption, we can consider tuning data caches.\n\n\n#### Enabling/disabling cache\n\n\nWe the  CQL to enable or disable caching by altering the cache\nproperty of a table. For instance, we use the `ALTER TABLE`\nstatement to enable the row cache for `watchlist`:\n\n\n``` {.programlisting .language-markup}\nALTER TABLE watchlist WITH caching=''ROWS_ONLY'';\n```\n\nOther available table caching options include\n`ALL` , `KEYS_ONLY` and `NONE`. They are\nquite self-explanatory and we do not go through each of them here.\n\n\n### Note\n\nFurther information about data caching can be found\nat\n<http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html>.\n\n\nSummary\n-------------------------\n\nThis lab highlights the most important aspects of deploying a\nCassandra cluster into the production environment. Cassandra can be\ntaught to understand the physical location of the nodes in the cluster\nin order to intelligently manage its availability, scalability and\nperformance. We deployed the Stock Screener Application to the\nproduction environment, though the scale is small. It is also valuable\nfor us to learn how to migrate legacy data from a non-production\nenvironment.\n\nWe then learned the basics of monitoring and performance tuning which\nare a must for a live running system. If you have experience in\ndeploying other database and system, you may well appreciate the\nneatness and simplicity of Cassandra.\n\n",
      "dateUpdated": "2020-06-07T12:14:26+0000",
      "dateFinished": "2020-06-07T12:14:26+0000",
      "dateStarted": "2020-06-07T12:14:26+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Lab 7. Deployment and Monitoring</h2>\n<p><strong>Note:</strong> We will run this example on single node like previous labs. This lab contains information regarding casssandra on multiple nodes instead.</p>\n<p>We have explored the development of the Stock Screener Application in<br />\nprevious labs; it is now time to consider how to deploy it in the<br />\nproduction environment. In this lab, we will discuss the most<br />\nimportant aspects of deploying a Cassandra database in production. These<br />\naspects include the selection of an appropriate combination of<br />\nreplication strategy, snitch, and replication factor to make up a<br />\nfault-tolerant, highly available cluster. Then we will demonstrate the<br />\nprocedure to migrate our Cassandra development database of the Stock<br />\nScreener Application to a production database. However, cluster<br />\nmaintenance is beyond the scope of this course.</p>\n<p>Moreover, a live production system that continuously operates certainly<br />\nrequires monitoring of its health status. We will cover the basic tools<br />\nand techniques of monitoring a Cassandra cluster, including the nodetool<br />\nutility, JMX and MBeans, and system log.</p>\n<p>Finally, we will explore ways of boosting the performance of Cassandra<br />\nother than using the defaults. Actually, performance tuning can be made<br />\nat several levels, from the lowest hardware and system configuration to<br />\nthe highest application coding techniques. We will focus on the <strong>Java<br />\nVirtual Machine</strong> (<strong>JVM</strong>)<br />\nlevel, because Cassandra heavily relies on its underlying performance.<br />\nIn addition, we will touch on how to tune caches for a table.</p>\n<h2>Replication strategies</h2>\n<p>This section is about the data replication<br />\nconfiguration of a Cassandra cluster. It will cover replication<br />\nstrategies, snitches, and the configuration of the cluster for the Stock<br />\nScreener Application.</p>\n<h3>Data replication</h3>\n<p>Cassandra, by design, can work in a huge cluster<br />\nacross multiple data centers all over the globe. In<br />\nsuch a distributed environment, network bandwidth and latency must be<br />\ncritically considered in the architecture, and careful planning in<br />\nadvance is required, otherwise it would lead to catastrophic<br />\nconsequences. The most obvious issue is the time clock<br />\nsynchronization&mdash;the genuine means of resolving transaction conflicts<br />\nthat can threaten data integrity in the whole cluster. Cassandra<br />\nthe underlying<br />\noperating system platform to provide the time clock synchronization<br />\nservice. Furthermore, a node is highly likely to fail at some time and<br />\nthe cluster must be resilient to this typical node failure. These issues<br />\nhave to be thoroughly considered at the architecture level.</p>\n<p>Cassandra adopts data replication to tackle these issues, based on the<br />\nidea of using space in exchange of time. It simply consumes more storage<br />\nspace to make data replicas so as to minimize the complexities in<br />\nresolving the previously mentioned issues in a cluster.</p>\n<p>Data replication is configured by the so-called replication factor in a<br />\n<strong>keyspace</strong>. The replication factor<br />\nrefers to the total number of copies of each row across the cluster. So<br />\na replication factor of <code>1</code> (as seen in the examples in<br />\nprevious labs) means that there is only one copy of each row on one<br />\nnode. For a replication factor of <code>2</code> , two copies of each row<br />\nare on two different nodes. Typically, a replication factor of<br />\n<code>3</code> is sufficient in most production scenarios.</p>\n<p>All data replicas are equally important. There are neither master nor<br />\nslave replicas. So data replication does not have scalability issues.<br />\nThe replication factor can be increased as more nodes are added.<br />\nHowever, the replication factor should not be set to exceed the number<br />\nof nodes in the cluster.</p>\n<p>Another unique feature of Cassandra is its awareness of the physical<br />\nlocation of nodes in a cluster and their proximity to each other.<br />\nCassandra can be configured to know the layout of the data centers and<br />\nracks by a correct IP address assignment scheme. This setting is known<br />\nas replication strategy and Cassandra provides two choices for us:<br />\n<code>SimpleStrategy</code> and <code>NetworkTopologyStrategy</code>.</p>\n<h3>SimpleStrategy</h3>\n<p><code>SimpleStrategy</code> is  used on a single<br />\nmachine or on a cluster in a single data center. It<br />\nplaces the first replica on a node determined by the partitioner, and<br />\nthen the additional replicas are placed on the next nodes in a clockwise<br />\nfashion without considering the data center and rack locations. Even<br />\nthough this is the default replication strategy when creating a<br />\nkeyspace, if we ever intend to have more than one data center, we should<br />\nuse <code>NetworkTopologyStrategy</code> instead.</p>\n<h3>NetworkTopologyStrategy</h3>\n<p><code>NetworkTopologyStrategy</code> becomes<br />\naware of the locations of data centers and racks by<br />\nunderstanding the IP addresses of the nodes in the cluster. It places<br />\nreplicas in the same data center by the clockwise mechanism until the<br />\nfirst node in another rack is reached. It attempts to place replicas on<br />\ndifferent racks because the nodes in the same rack often fail at the<br />\nsame time due to power, network issues, air conditioning, and so on.</p>\n<p>As mentioned, Cassandra knows the physical location from the IP<br />\naddresses of the nodes. The mapping of the IP addresses to the data<br />\ncenters and racks is referred to as a<br />\n<strong>snitch</strong>. Simply put, a snitch determines which data<br />\ncenters and racks the nodes belong to. It optimizes read operations by<br />\nproviding information about the network topology to Cassandra such that<br />\nread requests can be routed efficiently. It also affects how replicas<br />\ncan be distributed in consideration of the physical location of data<br />\ncenters and racks.</p>\n<p>There are many types of snitches available for different scenarios and<br />\neach comes with its pros and cons. They are briefly described as<br />\nfollows:</p>\n<ul>\n<li>\n<p><code>SimpleSnitch</code>: This is used for<br />\nsingle data center deployments only</p>\n</li>\n<li>\n<p><code>DynamicSnitch</code>: This monitors the<br />\nperformance of read operations from different<br />\nreplicas, and chooses the best replica based on historical<br />\nperformance</p>\n</li>\n<li>\n<p><code>RackInferringSnitch</code>: This<br />\ndetermines the location of the nodes<br />\nby data center and rack corresponding to the IP addresses</p>\n</li>\n<li>\n<p><code>PropertyFileSntich</code>: This determines<br />\nthe locations of the nodes by data center and<br />\nrack</p>\n</li>\n<li>\n<p><code>GossipingPropertyFileSnitch</code>: This<br />\nautomatically updates all nodes using gossip<br />\nwhen adding new nodes</p>\n</li>\n<li>\n<p><code>EC2Snitch</code>: This  is used<br />\nwith Amazon EC2 in a single region</p>\n</li>\n<li>\n<p><code>EC2MultiRegionSnitch</code>: This is used<br />\nwith Amazon EC2 in multiple regions</p>\n</li>\n<li>\n<p><code>GoogleCloudSnitch</code>: This  is used<br />\nwith Google Cloud Platform across one or more<br />\nregions</p>\n</li>\n<li>\n<p><code>CloudstackSnitch</code>: This is  used for<br />\nApache Cloudstack environments</p>\n</li>\n</ul>\n<h3>Note</h3>\n<p><strong>Snitch Architecture</strong></p>\n<p>For more detailed information, please refer to the documentation made by<br />\nDataStax at<br />\n<a href=\"http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html\">http://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureSnitchesAbout_c.html</a>.</p>\n<p>The following figure illustrates an example of a cluster of eight nodes<br />\nin four racks across two data centers using<br />\n<code>RackInferringSnitch</code> and a replication factor of three per<br />\ndata center:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_01.jpg\" alt=\"\" /></p>\n<h3>Tip</h3>\n<p>All nodes in the cluster must use the same snitch setting.</p>\n<p>Let us look at the IP address assignment in <strong>Data Center 1</strong><br />\nfirst. The IP addresses are grouped and assigned in a top-down fashion.<br />\nAll the nodes in <strong>Data Center 1</strong> are in the same<br />\n<strong>123.1.0.0</strong> subnet. For those nodes in <strong>Rack<br />\n1</strong>, they are in the same <strong>123.1.1.0</strong> subnet.<br />\nHence, <strong>Node 1</strong> in <strong>Rack 1</strong> is assigned an IP<br />\naddress of <strong>123.1.1.1</strong> and <strong>Node 2</strong> in <strong>Rack<br />\n1</strong> is <strong>123.1.1.2</strong>. The same rule applies to<br />\n<strong>Rack 2</strong> such that the IP addresses of <strong>Node<br />\n1</strong> and <strong>Node 2</strong> in <strong>Rack 2</strong> are<br />\n<strong>123.1.2.1</strong> and <strong>123.1.2.2</strong>, respectively. For<br />\n<strong>Data Center 2</strong>, we just change the subnet of the data<br />\ncenter to <strong>123.2.0.0</strong> and the racks and nodes in <strong>Data<br />\nCenter 2</strong> are then changed similarly.</p>\n<p>The <code>RackInferringSnitch</code> deserves a more<br />\ndetailed explanation. It assumes that the network topology is known by<br />\nproperly assigned IP addresses based on the following rule:</p>\n<p><em>IP address = &lt;arbitrary octet&gt;.&lt;data center octet&gt;.&lt;rack<br />\noctet&gt;.&lt;node octet&gt;</em></p>\n<p>The formula for IP address assignment is shown in the previous<br />\nparagraph. With this very structured assignment of IP addresses,<br />\nCassandra can understand the physical location of all nodes in the<br />\ncluster.</p>\n<p>Another thing that we need to understand is the replication factor of<br />\nthe three replicas that are shown in the previous figure. For a cluster<br />\nwith <code>NetworkToplogyStrategy</code> , the replication factor is set<br />\non a per data center basis. So in our example, three replicas are placed<br />\nin <strong>Data Center 1</strong> as illustrated by the dotted arrows in<br />\nthe previous diagram. <strong>Data Center 2</strong> is another data<br />\ncenter that must have three replicas. Hence, there are six replicas in<br />\ntotal across the cluster.</p>\n<p>We will not go through every combination of the replication factor,<br />\nsnitch and replication strategy here, but we should now understand the<br />\nfoundation of how Cassandra makes use of them to flexibly deal with<br />\ndifferent cluster scenarios in real-life production.</p>\n<h3>Setting up the cluster for Stock Screener Application</h3>\n<p>Let us return to the Stock Screener Application.<br />\nThe cluster it runs on ,<br />\n<em>Enhancing a Version</em>, is a single-node cluster. In<br />\nthis section, we will set up a cluster of two nodes<br />\nthat can be used in small-scale production. We will also migrate the<br />\nexisting data in the development database to the new fresh production<br />\ncluster. It should be noted that for quorum reads/writes, it&rsquo;s usually<br />\nbest practice to use an odd number of nodes.</p>\n<h4>System and network configuration</h4>\n<p>The steps of installation and setup of the<br />\noperating system and network configuration are assumed to be done.<br />\nMoreover, both nodes should have Cassandra freshly installed. The system<br />\nconfiguration of the two nodes is identical and shown as follows:</p>\n<ul>\n<li>\n<p>OS: Ubuntu 12.04 LTS 64-bit</p>\n</li>\n<li>\n<p>Processor: Intel Core i7-4771 CPU @3.50GHz x 2</p>\n</li>\n<li>\n<p>Memory: 2 GB</p>\n</li>\n<li>\n<p>Disk: 20 GB</p>\n</li>\n</ul>\n<h4>Global settings</h4>\n<p>The  <strong>Test<br />\nCluster</strong>, in which both the<br />\n<strong>ubtc01</strong> and <strong>ubtc02</strong> nodes are in the same rack, <code>RACK1</code> , and in the<br />\nsame data center, <code>NY1</code>. The logical architecture of the<br />\ncluster to be set up is depicted in the following diagram:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_02.jpg\" alt=\"\" /></p>\n<p>In order to configure a Cassandra cluster, we need to modify a few<br />\nproperties in the main configuration file, <code>cassandra.yaml</code> ,<br />\nfor Cassandra. Depending on the installation method of Cassandra,<br />\n<code>cassandra.yaml</code> is located in different directories:</p>\n<ul>\n<li>\n<p>Package installation: <code>/etc/cassandra/</code></p>\n</li>\n<li>\n<p>Tarball installation: <code>&lt;install_location&gt;/conf/</code><br />\n:::</p>\n</li>\n</ul>\n<p>The first thing to do is to set the properties in<br />\n<code>cassandra.yaml</code> for each node. As the system configuration of<br />\nboth nodes is the same, the following modification on<br />\n<code>cassandra.yaml</code> settings is identical to them:</p>\n<pre><code class=\"language-{.programlisting\">-seeds: ubtc01\nlisten_address:\nrpc_address: 0.0.0.0\nendpoint_snitch: GossipingPropertyFileSnitch\nauto_bootstrap: false\n</code></pre>\n<p>The reason for using <code>GossipingPropertyFileSnitch</code> is that we<br />\nwant the Cassandra cluster to automatically update all nodes with the<br />\ngossip protocol when adding a new node.</p>\n<p>Apart from <code>cassandra.yaml</code> , we also need to modify the data<br />\ncenter and rack properties in <code>cassandra-rackdc.properties</code> in<br />\nthe same location as <code>cassandra.yaml</code>. In our case, the data<br />\ncenter is <code>NY1</code> and the rack is <code>RACK1</code> , as shown in<br />\nthe following code:</p>\n<pre><code class=\"language-{.programlisting\">dc=NY1\nrack=RACK1\n</code></pre>\n<h4>Configuration procedure</h4>\n<p>The configuration procedure of the cluster (refer<br />\nto the following bash shell scripts: <code>setup_ubtc01.sh</code> and<br />\n<code>setup_ubtc02.sh</code>) is enumerated as follows:</p>\n<ol>\n<li>\n<p>Stop Cassandra service:</p>\n<pre><code class=\"language-{.programlisting\">ubtc01:~$ sudo service cassandra stop\nubtc02:~$ sudo service cassandra stop\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Remove the system keyspace:</p>\n<pre><code class=\"language-{.programlisting\">ubtc01:~$ sudo rm -rf /var/lib/cassandra/data/system/*\nubtc02:~$ sudo rm -rf /var/lib/cassandra/data/system/*\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Modify <code>cassandra.yaml</code> and<br />\n<code>cassandra-rackdc.properties</code> in both nodes based on the<br />\nglobal settings as specified in the previous section</p>\n</li>\n<li>\n<p>Start the seed node <code>ubtc01</code> first:</p>\n<pre><code class=\"language-{.programlisting\">ubtc01:~$ sudo service cassandra start\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Then start <code>ubtc02</code>:</p>\n<pre><code class=\"language-{.programlisting\">ubtc02:~$ sudo service cassandra start\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Wait for a minute and check if <code>ubtc01</code> and<br />\n<code>ubtc02</code> are both up and running:</p>\n<pre><code class=\"language-{.programlisting\">ubtc01:~$ nodetool status\n</code></pre>\n<p>:::<br />\n:::</p>\n</li>\n</ol>\n<p>A successful result of setting up the cluster<br />\nshould resemble something similar to the following screenshot, showing<br />\nthat both nodes are up and running:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_03.jpg\" alt=\"\" /></p>\n<h4>Legacy data migration procedure</h4>\n<p>We now have the cluster ready but it is empty. We<br />\ncan simply rerun the Stock Screener Application to download and fill in<br />\nthe production database again. Alternatively, we can migrate the<br />\nhistorical prices collected in the development single-node cluster to<br />\nthis production cluster. In the case of the latter approach, the<br />\nfollowing procedure can help us ease the  data<br />\nmigration task:</p>\n<ol>\n<li>\n<p>Take a snapshot of the <code>packcdma</code> keyspace in the<br />\ndevelopment database (ubuntu is the hostname of the development<br />\nmachine):</p>\n<pre><code class=\"language-{.programlisting\">ubuntu:~$ nodetool snapshot fenagocdma\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Record the snapshot directory, in this example,<br />\n<strong>1412082842986</strong></p>\n</li>\n<li>\n<p>To play it safe, copy all SSTables under the snapshot directory to a<br />\ntemporary location, say <code>~/temp/</code>:</p>\n<pre><code class=\"language-{.programlisting\">ubuntu:~$ mkdir ~/temp/\nubuntu:~$ mkdir ~/temp/fenagocdma/\nubuntu:~$ mkdir ~/temp/fenagocdma/alert_by_date/\nubuntu:~$ mkdir ~/temp/fenagocdma/alertlist/\nubuntu:~$ mkdir ~/temp/fenagocdma/quote/\nubuntu:~$ mkdir ~/temp/fenagocdma/watchlist/\nubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/alert_by_date/snapshots/1412082842986/* ~/temp/fenagocdma/alert_by_date/\nubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/alertlist/snapshots/1412082842986/* ~/temp/fenagocdma/alertlist/\nubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/quote/snapshots/1412082842986/* ~/temp/fenagocdma/quote/\nubuntu:~$ sudo cp -p /var/lib/cassandra/data/fenagocdma/watchlist/snapshots/1412082842986/* ~/temp/fenagocdma/watchlist/\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Open cqlsh to connect to <code>ubtc01</code> and create a keyspace<br />\nwith the appropriate replication strategy in the production cluster:</p>\n<pre><code class=\"language-{.programlisting\">ubuntu:~$ cqlsh ubtc01\ncqlsh&gt; CREATE KEYSPACE fenagocdma WITH replication = {'class': 'NetworkTopologyStrategy',  'NY1': '2'};\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Create the <code>alert_by_date</code> , <code>alertlist</code> ,<br />\n<code>quote</code> , and <code>watchlist</code> tables:</p>\n<pre><code class=\"language-{.programlisting\">cqlsh&gt; CREATE TABLE fenagocdma.alert_by_date (\n  price_time timestamp,\n  symbol varchar,\n  signal_price float,\n  stock_name varchar,\n  PRIMARY KEY (price_time, symbol));\ncqlsh&gt; CREATE TABLE fenagocdma.alertlist (\n  symbol varchar,\n  price_time timestamp,\n  signal_price float,\n  stock_name varchar,\n  PRIMARY KEY (symbol, price_time));\ncqlsh&gt; CREATE TABLE fenagocdma.quote (\n  symbol varchar,\n  price_time timestamp,\n  close_price float,\n  high_price float,\n  low_price float,\n  open_price float,\n  stock_name varchar,\n  volume double,\n  PRIMARY KEY (symbol, price_time));\ncqlsh&gt; CREATE TABLE fenagocdma.watchlist (\n  watch_list_code varchar,\n  symbol varchar,\n  PRIMARY KEY (watch_list_code, symbol));\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Load the SSTables back to the production<br />\ncluster using the <code>sstableloader</code> utility:</p>\n<pre><code class=\"language-{.programlisting\">ubuntu:~$ cd ~/temp\nubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/alert_by_date\nubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/alertlist\nubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/quote\nubuntu:~/temp$ sstableloader -d ubtc01 fenagocdma/watchlist\n</code></pre>\n<p>:::</p>\n</li>\n<li>\n<p>Check the legacy data in the production database on<br />\n<code>ubtc02</code>:</p>\n<pre><code class=\"language-{.programlisting\">cqlsh&gt; select * from fenagocdma.alert_by_date;\ncqlsh&gt; select * from fenagocdma.alertlist;\ncqlsh&gt; select * from fenagocdma.quote;\ncqlsh&gt; select * from fenagocdma.watchlist;\n</code></pre>\n<p>:::<br />\n:::</p>\n</li>\n</ol>\n<p>Although the previous steps look complicated, it is<br />\nnot difficult to understand what they want to achieve. It should be<br />\nnoted that we have set the replication factor per data center as<br />\n<code>2</code> to provide data redundancy on both nodes, as shown in the<br />\n<code>CREATE KEYSPACE</code> statement. The replication factor can be<br />\nchanged in future if needed.</p>\n<h4>Deploying the Stock Screener Application</h4>\n<p>As we have set up the production cluster and moved<br />\nthe legacy data into it, it is time to deploy the Stock Screener<br />\nApplication. The only thing needed to modify is the code to establish<br />\nCassandra connection to the production cluster. This is very easy to do<br />\nwith Python. The code in <code>lab06_006.py</code> is modified to<br />\nwork with the production cluster as <code>lab07_001.py</code>. A new<br />\ntest case named <code>testcase003()</code> is created to replace<br />\n<code>testcase002()</code>. To save pages, the complete source code of<br />\n<code>lab07_001.py</code> is not shown here; only the<br />\n<code>testcase003()</code> function is depicted as follows:</p>\n<pre><code class=\"language-{.programlisting\"># -*- coding: utf-8 -*-\n# program: lab07_001.py\n\n# other functions are not shown for brevity\n\ndef testcase003():\n    ## create Cassandra instance with multiple nodes\n    cluster = Cluster(['ubtc01', 'ubtc02'])\n    \n    ## establish Cassandra connection, using local default\n    session = cluster.connect('fenagocdma')\n    \n    start_date = datetime.datetime(2012, 6, 28)\n    end_date = datetime.datetime(2013, 9, 28)\n    \n    ## load the watch list\n    stocks_watched = load_watchlist(session, &quot;WS01&quot;)\n    \n    for symbol in stocks_watched:\n        ## retrieve data\n        data = retrieve_data(session, symbol, start_date, end_date)\n        \n        ## compute 10-Day SMA\n        data = sma(data, 10)\n        \n        ## generate the buy-and-hold signals\n        alerts = signal_close_higher_than_sma10(data)\n        \n        ## save the alert list\n        for index, r in alerts.iterrows():\n            insert_alert(session, symbol, index, \\\n                         Decimal(r['close_price']), \\\n                         r['stock_name'])\n    \n    ## close Cassandra connection\n    cluster.shutdown()\n\ntestcase003()\n</code></pre>\n<p>The cluster connection code right at the beginning<br />\nof the <code>testcase003()</code> function is passed with an array of the<br />\nnodes to be connected (<code>ubtc01</code> and <code>ubtc02</code>). Here<br />\nwe adopted the default <code>RoundRobinPolicy</code> as the connection<br />\nload balancing policy. It is used to decide how to distribute requests<br />\namong all possible coordinator nodes in the cluster. There are many<br />\nother options which are described in the driver API documentation.</p>\n<h3>Note</h3>\n<p><strong>Cassandra Driver 2.1 Documentation</strong></p>\n<p>For the complete API documentation of the Python<br />\nDriver 2.1 for Apache Cassandra, you can refer to<br />\n<a href=\"http://datastax.github.io/python-driver/api/index.html\">http://datastax.github.io/python-driver/api/index.html</a>.</p>\n<h2>Monitoring</h2>\n<p>As the  application system goes live, we need to<br />\nmonitor its health day-by-day. Cassandra provides a number of tools for<br />\nthis purpose. We will introduce some of them with pragmatic<br />\nrecommendations. It is remarkable that each operating system also<br />\nprovides a bunch of tools and utilities for<br />\nmonitoring, for example, <code>top</code> , <code>df</code> , <code>du</code><br />\non Linux and Task Manager on Windows. However, they are beyond the scope<br />\nof this course.</p>\n<h3>Nodetool</h3>\n<p>The  utility should<br />\nnot be new to us. It is a command-line interface used to monitor<br />\nCassandra and perform routine database operations. It includes the most<br />\nimportant metrics for tables, server, and compaction statistics, and<br />\nother useful commands for administration.</p>\n<p>Here are the most commonly used <code>nodetool</code> options:</p>\n<ul>\n<li>\n<p><code>status</code>: This  provides a concise<br />\nsummary of the cluster, such as the state, load, and IDs</p>\n</li>\n<li>\n<p><code>netstats</code>: This gives the network<br />\ninformation for a node, focusing on read repair operations</p>\n</li>\n<li>\n<p><code>info</code>: This gives valuable node<br />\ninformation including token, on disk load, uptime, Java heap memory<br />\nusage, key cache, and row cache</p>\n</li>\n<li>\n<p><code>tpstats</code>: This provides statistics<br />\nabout the number of active, pending, and completed tasks for each<br />\nstage of Cassandra operations by thread pool</p>\n</li>\n<li>\n<p><code>cfstats</code>: This gets the statistics<br />\nabout one or more tables, such as read-and-write counts and<br />\nlatencies, metrics about SSTable, memtable, bloom filter, and<br />\ncompaction.</p>\n</li>\n</ul>\n<h3>Note</h3>\n<p>A detailed documentation of nodetool can be<br />\nreferred to at<br />\n<a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html\">http://www.datastax.com/documentation/cassandra/2.0/cassandra/tools/toolsNodetool_r.html</a>.</p>\n<h3>JMX and MBeans</h3>\n<p>Cassandra written in the<br />\nJava language and so it natively supports <strong>Java<br />\nManagement Extensions</strong> (<strong>JMX</strong>). We may use<br />\nJConsole, a JMX-compliant tool, to monitor<br />\nCassandra.</p>\n<h3>Note</h3>\n<p><strong>JConsole</strong></p>\n<p>JConsole is included with Sun JDK 5.0 and higher<br />\nversions. However, it consumes a significant amount of system resources.<br />\nIt is recommended that you run it on a remote machine rather than on the<br />\nsame host as a Cassandra node.</p>\n<p>We can launch JConsole by typing<br />\n<code>jconsole</code> in a terminal. Assuming that we want to monitor the<br />\nlocal node, when the <strong>New Connection</strong> dialog box pops up,<br />\nwe type <code>localhost:7199</code> (<code>7199</code> is the port number<br />\nof JMX) in the <strong>Remote Process</strong> textbox, as depicted in the<br />\nfollowing screenshot:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_04.jpg\" alt=\"\" /></p>\n<p>After having connected to the local Cassandra instance, we will see a<br />\nwell-organized GUI showing six separate tabs placed horizontally on the<br />\ntop, as seen in the following screenshot:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_05.jpg\" alt=\"\" /></p>\n<p>The tabs of the GUI are explained as follows:</p>\n<ul>\n<li>\n<p><strong>Overview</strong>: This displays overview information about<br />\nthe JVM and monitored values</p>\n</li>\n<li>\n<p><strong>Memory</strong>: This displays information about heap and<br />\nnon-heap memory usage and garbage collection metrics</p>\n</li>\n<li>\n<p><strong>Threads</strong>: This displays information about thread use</p>\n</li>\n<li>\n<p><strong>Classes</strong>: This displays information about class<br />\nloading</p>\n</li>\n<li>\n<p><strong>VM Summary</strong>: This displays information about the JVM</p>\n</li>\n<li>\n<p><strong>MBeans</strong>: This displays information about specific<br />\nCassandra metrics and operations<br />\n:::</p>\n</li>\n</ul>\n<p>Furthermore, Cassandra provides five MBeans for<br />\nJConsole. They are briefly introduced as follows:</p>\n<ul>\n<li>\n<p><code>org.apache.cassandra.db</code>: This<br />\nincludes caching, table metrics, and compaction</p>\n</li>\n<li>\n<p><code>org.apache.cassandra.internal</code>: These<br />\nare internal server operations such as gossip and hinted<br />\nhandoff</p>\n</li>\n<li>\n<p><code>org.apache.cassandra.metrics</code>: These<br />\nare various metrics of the Cassandra instance such as cache and<br />\ncompaction</p>\n</li>\n<li>\n<p><code>org.apache.cassandra.net</code>: This has<br />\nInter-node communication including FailureDetector, MessagingService<br />\nand StreamingService</p>\n</li>\n<li>\n<p><code>org.apache.cassandra.request</code>: These<br />\ninclude tasks related to read, write, and replication operations</p>\n</li>\n</ul>\n<h3>Note</h3>\n<p><strong>MBeans</strong></p>\n<p>An <strong>Managed Bean</strong> (<strong>MBean</strong>) is a Java object that represents a manageable resource such as<br />\nan application, a service, a component, or a device running in the JVM.<br />\nIt can be used to collect statistics on concerns such as performance,<br />\nresource usage, or problems, for getting and setting application<br />\nconfigurations or properties, and notifying events like faults or state<br />\nchanges.</p>\n<h3>The system log</h3>\n<p>The most  yet the<br />\nmost powerful, monitoring tool is Cassandra&rsquo;s system log. The default<br />\nlocation of the system log is named <code>system.log</code> under<br />\n<code>/var/log/cassandra/</code>. It is simply a text file and can be<br />\nviewed or edited by any text editor. The following screenshot shows an<br />\nextract of <code>system.log</code>:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_06.jpg\" alt=\"\" /></p>\n<p>This piece log looks long<br />\nand weird. However, if you are a Java developer and you are familiar<br />\nwith the standard log library, Log4j, it is pretty straightforward. The<br />\nbeauty of Log4j is the provision of different log levels for us to<br />\ncontrol the granularity of the log statements to be recorded in<br />\n<code>system.log</code>. As shown in the previous figure, the first word<br />\nof each line is <code>INFO</code> , meaning that the log statement is a<br />\npiece of information. Other log level choices include <code>FATAL</code> ,<br />\n<code>ERROR</code> , <code>WARN</code> , <code>DEBUG</code> , and<br />\n<code>TRACE</code> , from the least verbose to the most verbose.</p>\n<p>The system log is very valuable in troubleshooting problems as well. We<br />\nmay increase the log level to <code>DEBUG</code> or <code>TRACE</code> for<br />\ntroubleshooting. However, running a production Cassandra cluster in the<br />\n<code>DEBUG</code> or <code>TRACE</code> mode will degrade its performance<br />\nsignificantly. We must use them with great care.</p>\n<p>We can change the standard log level in Cassandra by adjusting the<br />\n<code>log4j.rootLogger</code> property in<br />\n<code>log4j-server.properties</code> in the Cassandra configuration<br />\ndirectory. The following screenshot shows the content of<br />\n<code>log4j-server.properties</code> in ubtc02:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_07.jpg\" alt=\"\" /></p>\n<p>It is important to mention that<br />\n<code>system.log</code> and <code>log4j-server.properties</code> are only<br />\nresponsible for a single node. For a cluster of two nodes, we<br />\nwill have two <code>system.log</code> and two<br />\n<code>log4j-server.properties</code> on the respective nodes.</p>\n<h2>Performance tuning</h2>\n<p>Performance tuning is a large and complex topic<br />\nthat in itself can be a whole course. We can only scratch the surface of<br />\nit in this short section. Similar to monitoring in the last section,<br />\noperating system-specific performance tuning techniques are beyond the<br />\nscope of this course.</p>\n<h3>Java virtual machine</h3>\n<p>Based on the information given by the monitoring<br />\ntools and the system log, we can discover<br />\nopportunities for performance tuning. The first things we usually watch<br />\nare the Java heap memory and garbage collection. JVM&rsquo;s configuration<br />\nsettings are controlled in the environment settings<br />\nfile for Cassandra, <code>cassandra-env.sh</code> , located in<br />\n<code>/etc/cassandra/</code>. An example is shown in the following<br />\nscreenshot:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_08.jpg\" alt=\"\" /></p>\n<p>Basically, it already has the boilerplate options<br />\ncalculated to be optimized for the host system. It is also accompanied<br />\nwith explanation for us to tweak specific JVM parameters and the startup<br />\noptions of a Cassandra instance when we experience real issues;<br />\notherwise, these boilerplate options should not be altered.</p>\n<h3>Note</h3>\n<p>A detailed documentation on how to tune JVM for<br />\nCassandra can be found at<br />\n<a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html</a>.</p>\n<h3>Caching</h3>\n<p>Another we should pay<br />\nattention to is caching. Cassandra includes integrated caching and<br />\ndistributes cache data around the cluster. For a cache specific to a<br />\ntable, we will focus on the partition key cache and the row cache.</p>\n<h4>Partition key cache</h4>\n<p>The partition key cache, or key cache for short, is a cache of the partition index for a<br />\ntable. Using the key cache saves processor time and memory. However,<br />\nenabling just the key cache makes the disk activity actually read the<br />\nrequested data rows.</p>\n<h4>Row cache</h4>\n<p>The  is similar to a<br />\ntraditional cache. When a row is accessed, the entire row is pulled into<br />\nmemory, merging from multiple SSTables when required, and cached. This<br />\nprevents Cassandra from retrieving that row using disk I/O again, which<br />\ncan tremendously improve read performance.</p>\n<p>When both row cache and partition key cache are configured, the row<br />\ncache returns results whenever possible. In the event of a row cache<br />\nmiss, the partition key cache might still provide a hit that makes the<br />\ndisk seek much more efficient.</p>\n<p>However, there is one caveat. Cassandra caches all the rows of a<br />\npartition when reading that partition. So if the partition is large or<br />\nonly a small portion of the partition is read every time, the row cache<br />\nmight not be beneficial. It is very easy to be misused and consequently<br />\nthe JVM will be exhausted, causing Cassandra to fail. That is why the<br />\nrow cache is disabled by default.</p>\n<h3>Note</h3>\n<p>We usually enable either the key or row cache for a table, not both at<br />\nthe same time.</p>\n<h4>Monitoring cache</h4>\n<p>Either the <code>nodetool info</code> command or JMX MBeans can provide assistance in monitoring<br />\ncache. We should make changes to cache options in small, incremental<br />\nadjustments, and then monitor the effects of each change using the<br />\nnodetool utility. The last two lines of output of the<br />\n<code>nodetool info</code> command, as seen in the following figure,<br />\ncontain the <code>Row Cache</code> and <code>Key Cache</code> metrics of<br />\n<code>ubtc02</code>:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_07_09.jpg\" alt=\"\" /></p>\n<p>In the of high memory<br />\nconsumption, we can consider tuning data caches.</p>\n<h4>Enabling/disabling cache</h4>\n<p>We the  CQL to enable or disable caching by altering the cache<br />\nproperty of a table. For instance, we use the <code>ALTER TABLE</code><br />\nstatement to enable the row cache for <code>watchlist</code>:</p>\n<pre><code class=\"language-{.programlisting\">ALTER TABLE watchlist WITH caching=''ROWS_ONLY'';\n</code></pre>\n<p>Other available table caching options include<br />\n<code>ALL</code> , <code>KEYS_ONLY</code> and <code>NONE</code>. They are<br />\nquite self-explanatory and we do not go through each of them here.</p>\n<h3>Note</h3>\n<p>Further information about data caching can be found<br />\nat<br />\n<a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html\">http://www.datastax.com/documentation/cassandra/2.0/cassandra/operations/ops_configuring_caches_c.html</a>.</p>\n<h2>Summary</h2>\n<p>This lab highlights the most important aspects of deploying a<br />\nCassandra cluster into the production environment. Cassandra can be<br />\ntaught to understand the physical location of the nodes in the cluster<br />\nin order to intelligently manage its availability, scalability and<br />\nperformance. We deployed the Stock Screener Application to the<br />\nproduction environment, though the scale is small. It is also valuable<br />\nfor us to learn how to migrate legacy data from a non-production<br />\nenvironment.</p>\n<p>We then learned the basics of monitoring and performance tuning which<br />\nare a must for a live running system. If you have experience in<br />\ndeploying other database and system, you may well appreciate the<br />\nneatness and simplicity of Cassandra.</p>\n\n</div>"
          }
        ]
      }
    },
    {
      "text": "# -*- coding: utf-8 -*-\r\n# program: lab07_001.py\r\n\r\n## import Cassandra driver library\r\nfrom cassandra.cluster import Cluster\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport datetime\r\n\r\n## import Cassandra BatchStatement library\r\nfrom cassandra.query import BatchStatement\r\nfrom decimal import *\r\n\r\n## function to insert historical data into table quote\r\n## ss: Cassandra session\r\n## sym: stock symbol\r\n## sd: start date\r\n## ed: end date\r\n## return a DataFrame of stock quote\r\ndef retrieve_data(ss, sym, sd, ed):\r\n    ## CQL to select data, ? is the placeholder for parameters\r\n    select_cql = \"SELECT * FROM quote WHERE symbol=? \" + \\\r\n                 \"AND price_time >= ? AND price_time <= ?\"\r\n\r\n    ## prepare select CQL\r\n    select_stmt = ss.prepare(select_cql)\r\n\r\n    ## execute the select CQL\r\n    result = ss.execute(select_stmt, [sym, sd, ed])\r\n\r\n    ## initialize an index array\r\n    idx = np.asarray([])\r\n\r\n    ## initialize an array for columns\r\n    cols = np.asarray([])\r\n\r\n    ## loop thru the query resultset to make up the DataFrame\r\n    for r in result:\r\n        idx = np.append(idx, [r.price_time])\r\n        cols = np.append(cols, [r.open_price, r.high_price, \\\r\n                         r.low_price, r.close_price, \\\r\n                         r.volume, r.stock_name])\r\n\r\n    ## reshape the 1-D array into a 2-D array for each day\r\n    cols = cols.reshape(idx.shape[0], 6)\r\n\r\n    ## convert the arrays into a pandas DataFrame\r\n    df = pd.DataFrame(cols, index=idx, \\\r\n                      columns=['open_price', 'high_price', \\\r\n                      'low_price', 'close_price', \\\r\n                      'volume', 'stock_name'])\r\n    return df\r\n\r\n## function to compute a Simple Moving Average on a DataFrame\r\n## d: DataFrame\r\n## prd: period of SMA\r\n## return a DataFrame with an additional column of SMA\r\ndef sma(d, prd):\r\n    d['sma'] = d['close_price'].rolling(window=prd).mean()\r\n    return d\r\n\r\n## function to apply screening rule to generate buy signals\r\n## screening rule, Close > 10-Day SMA\r\n## d: DataFrame\r\n## return a DataFrame containing buy signals\r\ndef signal_close_higher_than_sma10(d):\r\n    return d[d.close_price.eq(d.sma)]\r\n\r\n## function to retrieve watchlist\r\n## ss: Cassandra session\r\n## ws: watchlist code\r\ndef load_watchlist(ss, ws):\r\n    ## CQL to select data, ? is the placeholder for parameters\r\n    select_cql = \"SELECT symbol FROM watchlist \" + \\\r\n                 \"WHERE watch_list_code=?\"\r\n\r\n    ## prepare select CQL\r\n    select_stmt = ss.prepare(select_cql)\r\n\r\n    ## execute the select CQL\r\n    result = ss.execute(select_stmt, [ws])\r\n\r\n    ## initialize the stock array\r\n    stw = []\r\n\r\n    ## loop thru the query resultset to make up the DataFrame\r\n    for r in result:\r\n        stw.append(r.symbol)\r\n\r\n    return stw\r\n\r\n## function to insert historical data into table quote\r\n## ss: Cassandra session\r\n## sym: stock symbol\r\n## d: standardized DataFrame containing historical data\r\n## sn: stock name\r\ndef insert_alert(ss, sym, sd, cp, sn):\r\n    ## CQL to insert data, ? is the placeholder for parameters\r\n    insert_cql1 = \"INSERT INTO alertlist (\" + \\\r\n                 \"symbol, price_time, signal_price, stock_name\" +\\\r\n                 \") VALUES (?, ?, ?, ?)\"\r\n\r\n    ## CQL to insert data, ? is the placeholder for parameters\r\n    insert_cql2 = \"INSERT INTO alert_by_date (\" + \\\r\n                 \"symbol, price_time, signal_price, stock_name\" +\\\r\n                 \") VALUES (?, ?, ?, ?)\"\r\n\r\n    ## prepare the insert CQL as it will run repeatedly\r\n    insert_stmt1 = ss.prepare(insert_cql1)\r\n    insert_stmt2 = ss.prepare(insert_cql2)\r\n\r\n    ## set decimal places to 4 digits\r\n    getcontext().prec = 4\r\n\r\n    ## begin a batch\r\n    batch = BatchStatement()\r\n    \r\n    ## add insert statements into the batch\r\n    batch.add(insert_stmt1, [sym, sd, cp, sn])\r\n    batch.add(insert_stmt2, [sym, sd, cp, sn])\r\n    \r\n    ## execute the batch\r\n    ss.execute(batch)\r\n\r\ndef testcase003():\r\n\r\n    ######################################################################## \r\n    # We will be running the example on single node in the lab environment # \r\n    ######################################################################## \r\n\r\n    ## create Cassandra instance\r\n    cluster = Cluster()\r\n\r\n    ## create Cassandra instance with multiple nodes\r\n    # cluster = Cluster(['ubtc01', 'ubtc02'])\r\n    \r\n    ## establish Cassandra connection, using local default\r\n    session = cluster.connect('fenagocdma')\r\n    \r\n    start_date = datetime.datetime(2012, 6, 28)\r\n    end_date = datetime.datetime(2013, 9, 28)\r\n    \r\n    ## load the watch list\r\n    stocks_watched = load_watchlist(session, \"WS01\")\r\n    \r\n    for symbol in stocks_watched:\r\n        ## retrieve data\r\n        data = retrieve_data(session, symbol, start_date, end_date)\r\n        \r\n        ## compute 10-Day SMA\r\n        data = sma(data, 10)\r\n        \r\n        ## generate the buy-and-hold signals\r\n        alerts = signal_close_higher_than_sma10(data)\r\n        \r\n        ## save the alert list\r\n        for index, r in alerts.iterrows():\r\n            insert_alert(session, symbol, index, \\\r\n                         Decimal(r['close_price']), \\\r\n                         r['stock_name'])\r\n    \r\n    ## close Cassandra connection\r\n    cluster.shutdown()\r\n\r\ntestcase003()\r\n",
      "user": "anonymous",
      "dateUpdated": "2020-06-07T12:08:00+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1591531680695_-1246225855",
      "id": "paragraph_1589809690178_1693822238",
      "dateCreated": "2020-06-07T12:08:00+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:1402"
    },
    {
      "user": "anonymous",
      "dateUpdated": "2020-06-07T12:08:00+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1591531680697_-1817170934",
      "id": "paragraph_1589809697620_-498148424",
      "dateCreated": "2020-06-07T12:08:00+0000",
      "status": "READY",
      "$$hashKey": "object:1403"
    }
  ],
  "name": "lab_7",
  "id": "2FAZBVJXA",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/lab_7"
}