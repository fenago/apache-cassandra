{
  "paragraphs": [
    {
      "text": "%md\r\n\r\nLab 1. Bird's Eye View of Cassandra\r\n-------------------------------------------------\r\n\r\n\r\nImagine that we have turned back the clock to the 1990s and you an\r\napplication architect. Whenever you were required to select a suitable\r\ndatabase technology for your applications, what kind of database\r\ntechnology would you choose? I bet 95 percent (or more) of the time you\r\nwould select relational databases.\r\n\r\n**Relational databases** have been the\r\nmost dominating data management solution since the 1970s. At that time,\r\nthe application system was usually silo. The users of the application\r\nand their usage patterns were known and under control. The workload that\r\nhad to be catered for by the relational database could be determined and\r\nestimated. Apart from the workload consideration, the data model can\r\nalso be structured in normalized forms as recommended by the relational\r\ntheory. Moreover, relational databases provide many benefits such as\r\nsupport of transactions, data consistency, and isolation. Relational\r\ndatabases just fit perfectly for the purposes. Therefore, it is not\r\ndifficult to understand why the relational database has been so popular\r\nand why it is the de facto standard for persistent data stores in\r\napplication development.\r\n\r\nNonetheless, with the proliferation of the Internet and the numerous web\r\napplications running on it, the control of the users and their usage\r\npatterns (hence the scale), the workload generated, and the flexibility\r\nof the data model were gone. Typical examples of these web applications\r\nwere global e-commerce websites, social media sites, video community\r\nwebsites, and so on. They generated a tremendous amount of data in a\r\nvery short period of time. It should also be noted that the data\r\ngenerated by these applications were not only structured, but also\r\nsemi-structured and even unstructured. Since relational databases were\r\nthe de facto standard at that time, developers and architects did not\r\nhave many alternatives but were forced to tweak them to support these\r\nweb applications, even though they knew that relational databases were\r\nsuboptimal and had many limitations. It became apparent that a different\r\nkind of enabling technology should be found to break through the\r\nchallenges.\r\n\r\nWe are in an era of information explosion, as a result of the\r\never-increasing amount of user-generated data and content on the Web and\r\nmobile applications. The generated data is not only large in volume and\r\nfast in velocity but it is also diversified in variety. Such rapidly\r\ngrowing data of different varieties is often termed as **Big Data**.\r\n\r\nNo one has a clear, formal definition of Big Data. People, however,\r\nunanimously agree that the most fundamental characteristics of Big Data\r\nare related to large volume, high velocity, and great variety. Big Data\r\nimposes real, new challenges to the information systems that have\r\nadopted traditional ways of handling data. These systems are not\r\ndesigned for web-scale and for being enhanced to do so, cost\r\neffectively. Due to this, you might find yourself asking whether or not\r\nwe have any alternatives.\r\n\r\nChallenges come with opportunities on the flip side. A new breed of data\r\nmanagement products was born. The most recent answer to the question in\r\nthe last paragraph is NoSQL.\r\n\r\n\r\n\r\nWhat is NoSQL?\r\n--------------------------------\r\n\r\n\r\n\r\nThe need to tackle the Big Data challenges has led to\r\nthe emergence of new data management technologies and techniques. Such\r\ntechnologies and techniques are rather different from the ubiquitous\r\nrelational database technology that has been used for over 40 years.\r\nThey are collectively known as **NoSQL**.\r\n\r\nNoSQL is an umbrella term for the data stores that are not based on the\r\nrelational data model. It encompasses a great variety of many different\r\ndatabase technologies and products. As shown in the following figure,\r\nThe **Data Platforms Landscape Map**,\r\nthere are over 150 different database products that belong to the\r\nnon-relational school as mentioned in\r\n<http://nosql-database.org/>. Cassandra is one of the most popular ones.\r\nOther popular NoSQL database products are, just to name a few, MongoDB,\r\nRiak, Redis, Neo4j, so on and so forth.\r\n\r\n\r\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_01.jpg)\r\n\r\n:::\r\nThe Data Platforms Landscape Map (Source: 451 Research)\r\n:::\r\n\r\nSo, what kinds of benefits are provided by NoSQL? When compared to the\r\nrelational database, NoSQL overcomes the weaknesses that the relational\r\ndata model does not address well, which are as\r\n follows:\r\n\r\n\r\n-   Huge volume of structured, semi-structured, and unstructured data\r\n\r\n-   Flexible data model (schema) that is easy to change\r\n\r\n-   Scalability and performance for web-scale applications\r\n\r\n-   Lower cost\r\n\r\n-   Impedance mismatch between the relational data model and\r\n    object-oriented programming\r\n\r\n-   Built-in replication\r\n\r\n-   Support for agile software\r\n    development\r\n\r\n### Note\r\n\r\n**Limitations of NoSQL Databases**\r\n\r\nMany NoSQL databases do not support transactions.\r\nThey use replication extensively so that the data in the cluster might\r\nbe momentarily inconsistent (although it is eventually consistent). In\r\naddition, the range queries are not available in NoSQL databases.\r\nFurthermore, a flexible schema might lead to problems with efficient\r\nsearches.\r\n:::\r\n\r\nThe huge volume of structured, semi-structured, and unstructured data\r\nwas mentioned earlier. What I want to dive deeper into here is that\r\ndifferent NoSQL databases provide different solutions for each of them.\r\nThe primary factor to be considered is the NoSQL database type, which\r\nwill be introduced in the subsequent section.\r\n\r\nAll NoSQL databases provide a flexible data model\r\nthat is easy to change and some might be even schemaless. In a\r\nrelational database, the relational data model is called schema. You\r\nneed to understand the data to be stored in a relational database,\r\ndesign the data model according to the relational database theory, and\r\ndefine the schema upfront in the relational database before you can\r\nactually store data inside it. It is a very structured approach for\r\nstructured data. It is a prescriptive data modeling process. It is\r\nabsolutely fine if the data model is stable, because there are not many\r\nchanges required. But what if the data model keeps changing in the\r\nfuture and you do not know what needs to be changed? You cannot\r\nprescribe comprehensively in advance. It leads to many inevitable\r\nremedies; say, data patching for example, to change the schema.\r\n\r\nConversely, in NoSQL databases, you need not prescribe comprehensively.\r\nYou only need to describe what is to be stored. You are not bound by the\r\nrelational database theory. You are allowed to change the data model\r\nwhenever necessary. The data model is schemaless and is a living object.\r\nIt evolves as life goes on. It is a descriptive data modeling process.\r\n\r\nScalability and performance for web-scale applications refer to the\r\nability of the system to be scaled, preferably horizontally, to support\r\nweb-scale workloads without considerably deteriorating system\r\nperformance. Relational databases can only be scaled out to form a\r\ncluster consisting of a very small number of nodes. It implies the\r\nrather low ceiling imposed on these web-scale applications using\r\nrelational databases. In addition, changing the schema in a clustered\r\nrelational database is a big task of high complexity. The processing\r\npower required to do this is so significant that the system performance\r\ncannot be unaffected. Most NoSQL databases were created to serve\r\nweb-scale applications. They natively support horizontal scaling without\r\nvery little degrade on the performance.\r\n\r\nNow let us talk about money. Traditionally, most\r\nhigh-end relational databases are commercial products that demand their\r\nusers to pay huge software license fees. Besides, to run these high-end\r\nrelational databases, the underlying hardware servers are usually\r\nhigh-end as well. The result is that the hardware and software costs of\r\nrunning a powerful relational database are exceptionally large. In\r\ncontrast, NoSQL databases are open source and community-driven in a\r\nmajority, meaning that you need to pay the software license cost, which\r\nis an order of magnitude less than other databases. NoSQL databases are\r\nable to run on commodity machines that will lead to a possible churn, or\r\ncrashes. Therefore, the machines are usually configured to be a cluster.\r\nHigh-end hardware servers are not needed and so the hardware cost is\r\ntremendously reduced. It should be noted that when NoSQL databases are\r\nput into production, some cost of the support is still required but it\r\nis definitely much less when compared to that of commercial products.\r\n\r\nThere exists a generation gap between the relational data model and\r\nobject-oriented programming. The relational data model was the product\r\nof 1970s, whereas object-oriented programming became very popular in\r\n1990s. The root cause, known as impedance mismatch, is an inherent\r\ndifficulty of representing a record or a table in a relational data\r\nmodel with the object-oriented model. Although there are resolutions for\r\nthis difficulty, most application developers still feel very frustrated\r\nto bring the two together.\r\n\r\n\r\n### Note\r\n\r\n**Impedance Mismatch**\r\n\r\nImpedance mismatch is the difference between the\r\nrelational model and the in-memory data structures that are usually\r\nencountered in object-oriented programming languages.\r\n:::\r\n\r\nBuilt-in replication is a feature that most NoSQL databases provide to\r\nsupport high availability in a cluster of many nodes. It is usually\r\nautomatic and transparent to the application developers. Such a feature\r\nis also available in relational databases, but the database\r\nadministrators must struggle to configure, manage, and operate it by\r\nthemselves.\r\n\r\nFinally, relational databases do not support agile software development\r\nvery well. Agile software development is iterative by nature. The\r\nsoftware architecture and data model emerge and evolve as the project\r\nproceeds in order to deliver the product incrementally. Hence, it is\r\nconceivable that the need of changing the data model to meet the new\r\nrequirements is inevitably frequent. Relational databases are structured\r\nand do not like changes. NoSQL can provide such flexibility for agile\r\nsoftware development teams by virtue of its schemaless characteristic.\r\nEven better, NoSQL databases usually allow the changes to be implemented\r\nin real time without any downtime.\r\n\r\n\r\n\r\n### NoSQL Database types\r\n\r\n\r\nNow you know the benefits of NoSQL databases, but\r\nthe products that fall under the NoSQL databases umbrella are quite\r\nvaried. How can you select the right one for yourself among so many\r\nNoSQL databases? The selection criteria of which NoSQL database fits\r\nyour needs is really dependent on the use cases at hand. The most\r\nimportant factor to consider here is the NoSQL database type, which can\r\nbe subdivided into four main categories:\r\n\r\n\r\n-   Key/value pair store\r\n\r\n-   Column-family store\r\n\r\n-   Document-based repository\r\n\r\n-   Graph database\r\n:::\r\n\r\nThe NoSQL database type dictates the data model that you can use. It is\r\nbeneficial to understand each of them deeper.\r\n\r\n\r\n\r\n#### Key/value pair store\r\n\r\n\r\nKey/value pair is the simplest NoSQL database type.\r\nKey/value store is similar to the concept of Windows registry, or in\r\nJava or C\\#, a map, a hash, a key/value pair. Each\r\ndata item is represented as an attribute name, also a key, together with\r\nits value. It is also the basic unit stored in the database. Examples of\r\nthe NoSQL databases of key/value pair type\r\nare **Amazon Dynamo**, **Berkeley\r\nDB**, **Voldemort** and\r\n**Riak**.\r\n\r\nInternally, key/value pairs are stored in a data structure called **hashmap**. Hashmap is popular because it\r\nprovides very good performance on accessing data. The key of a key/value\r\npair is unique and can be searched very quickly.\r\n\r\nKey/value pair can be stored and distributed in the disk storage as well\r\nas in memory. When used in memory, it can be used as a cache, which\r\ndepends on the caching algorithm, can considerably reduce disk I/O and\r\nhence boost up the performance significantly.\r\n\r\nOn the flip side, key/value pair has some drawbacks, such as lack of\r\nsupport of range queries, no way to operate on multiple keys\r\nsimultaneously, and possible issues with load balancing.\r\n\r\n\r\n#### Column-family store\r\n\r\n\r\nA column in this context is not equal to a column in\r\na relational table. In the NoSQL world, a column is a data structure\r\nthat contains a key, value, and timestamp. Thus, it can be regarded as a\r\ncombination of key/value pair and a timestamp.\r\nExamples are **Google BigTable**,\r\n**Apache Cassandra**, and  **Apache\r\nHBase**. They provide optimized performance for queries over\r\nvery large datasets.\r\n\r\nColumn-family store is basically a multi-dimensional\r\nmap. It stores columns of data together as a row, which is associated\r\nwith a row key. This contrasts with rows of data in a relational\r\ndatabase. Column-family store does not need to store null columns, as in\r\nthe case of a relational database and so it consumes much less disk\r\nspace. Moreover, columns are not bound by a rigid schema and you are not\r\nrequired to define the schema upfront.\r\n\r\nThe key component of a column is usually called the primary key or the\r\nrow key. Columns are stored in a sorted manner by the row key. All the\r\ndata belonging to a row key is stored together. As such, read and write\r\noperations of the data can be confined to a local node, avoiding\r\nunnecessary inter-node network traffic in a cluster. This mechanism\r\nmakes the data lookup and retrieval extremely efficient.\r\n\r\nObviously, a column-family store is not the best solution for systems\r\nthat require ACID transactions and it lacks the support for aggregate\r\nqueries provided by relational databases such as `SUM()`.\r\n\r\n\r\n#### Document-based repository\r\n\r\n\r\nDocument-based repository is designed for documents\r\nor semi-structured data. The basic unit of a\r\ndocument-based repository associates each key, a primary identifier,\r\nwith a complex data structure called a document. A document can contain\r\nmany different key-value pairs, or key-array pairs, or even nested\r\ndocuments. Therefore, document-based repository does not adhere to a\r\nschema. Examples are **MongoDB** and\r\n **CouchDB**.\r\n\r\nIn practice, a document is usually a loosely structured set of key/value\r\npairs in the form of  **JavaScript Object\r\nNotation** (**JSON**). Document-based repository\r\nmanages a document as a whole and avoids breaking up a document into\r\nfragments of key/value pairs. It also allows document properties to be\r\nassociated with a document.\r\n\r\nAs a document does not\r\nadhere to a fixed schema, the search performance is not guaranteed.\r\nThere are generally two approaches to query a document database. The\r\nfirst is to use materialized views (such as CouchDB) that are prepared\r\nin advance. The second is to use indexes defined on the document values\r\n(such as MongoDB) that behave in the same way as a relational database\r\nindex.\r\n\r\n\r\n#### Graph database\r\n\r\n\r\nGraph databases are designed for storing information\r\nabout networks, such as a social network. A graph is\r\nused to represent the highly connected network that is composed of nodes\r\nand their relationships. The nodes and relationships can have individual\r\nproperties. The prominent graph databases include\r\n**Neo4J** and  **FlockDB**.\r\n\r\nOwing to the unique characteristics of a graph, graph databases commonly\r\nprovide APIs for rapid traversal of graphs.\r\n\r\nGraph databases are particularly difficult to be scaled out with\r\nsharding because traversing a graph of the nodes on different machine\r\ndoes not provide a very good performance. It is also not a\r\nstraightforward operation to update all or a subset of the nodes at the\r\nsame time.\r\n\r\nSo far, you have grasped the fundamentals of the NoSQL family. Since\r\nthis course concentrates on Apache Cassandra and its data model, you need\r\nto know what Cassandra is and have a basic understanding of what its\r\narchitecture is, so that you can select and leverage the best available\r\noptions when you are designing your NoSQL data model and application.\r\n\r\n\r\nWhat is Cassandra?\r\n------------------------------------\r\n\r\n\r\n\r\n**Cassandra** can be simply described in\r\na single phrase: a massively scalable, highly available open source\r\nNoSQL database that is based on peer-to-peer architecture.\r\n\r\nCassandra is now 5 years old. It is an active open source project in the\r\nApache Software Foundation and therefore it is known as Apache Cassandra\r\nas well. Cassandra can manage huge volume of structured,\r\nsemi-structured, and unstructured data in a large distributed cluster\r\nacross multiple data centers. It provides linear scalability, high\r\nperformance, fault tolerance, and supports a very flexible data model.\r\n\r\n\r\n### Note\r\n\r\n**Netflix and Cassandra**\r\n\r\nOne very famous case study of Cassandra is\r\nNetflix's move to replace their Oracle SQL database to Cassandra\r\nrunning on cloud. As of March 2013, Netflix's Cassandra deployment\r\nconsists of 50 clusters with over 750 nodes. For more information,\r\nplease visit the case study at\r\n<http://www.datastax.com/wp-content/uploads/2011/09/CS-Netflix.pdf>.\r\n:::\r\n\r\nIn fact, many of the benefits that Cassandra provides are inherited from\r\nits two best-of-breed NoSQL parents, Google BigTable and Amazon Dynamo.\r\nBefore we go into the details of Cassandra's architecture, let us walk\r\nthrough each of them first.\r\n\r\n\r\n\r\n### Google BigTable\r\n\r\n\r\nGoogle BigTable is Google's core technology,\r\nparticularly addressing data persistence and management on web-scale. It\r\nruns the data stores for many Google applications, such as Gmail,\r\nYouTube, and Google Analytics. It was designed to be a web-scale data\r\nstore without sacrificing real-time responses. It has superb read and\r\nwrite performance, linear scalability, and continuous availability.\r\n\r\nGoogle BigTable is a sparse, distributed, persistent, multidimensional\r\nsorted map. The map is indexed by a row key.\r\n\r\nDespite the many benefits Google BigTable provides, the underlying\r\ndesign concept is really simple and elegant. It uses a persistent\r\ncommitlog for every data write request that it receives and then writes\r\nthe data into a memory store (acting as a cache). At regular intervals\r\nor when triggered by a particular event, the memory store is flushed to\r\npersistent disk storage by a background process. This persistent disk\r\nstorage is called **Sorted String Table**, or\r\n**SSTable**. The SSTable is immutable\r\nmeaning that once it has been written to a disk, it will never be\r\nchanged again. The word *sorted* means that the data inside\r\nthe SSTable is indexed and sorted and hence the data can be found very\r\nquickly. Since the write operation is log-based and memory-based, it\r\ndoes not involve any read operation, and therefore the write operation\r\ncan be extremely fast. If a failure happens, the commitlog can be used\r\nto replay the sequence of the write operations to merge the data that\r\npersists in the SSTables.\r\n\r\nRead operation is also very efficient by looking up the data in the\r\nmemory store and the indexed SSTables, which are then merged to return\r\nthe data.\r\n\r\nAll the above-mentioned Google BigTable brilliances\r\ndo come with a price. Because Google BigTable is distributed in nature,\r\nit is constrained by the famous *CAP theorem*, stating the\r\nrelationship among the three characteristics of a distributed system,\r\nnamely Consistency, Availability, and Partition-tolerance. In a\r\nnutshell, Google BigTable prefers Consistency and Partition-tolerance to\r\nAvailability.\r\n\r\n\r\n### Note\r\n\r\n**The CAP theorem**\r\n\r\nCAP is an acronym of the  three characteristics of a\r\ndistributed system: Consistency, Availability, and Partition-tolerance.\r\nConsistency means that all the nodes in a cluster see the same data at\r\nany point in time. Availability means that every request that is\r\nreceived by a non-failing node in the cluster must result in a response.\r\nPartition-tolerance means that a node can still function when\r\ncommunication with other groups of nodes is lost. Originating from Eric\r\nA. Brewer, the theorem states that in a distributed system, only two out\r\nof the three characteristics can be attained at the most.\r\n:::\r\n\r\nGoogle BigTable has trouble with Availability while keeping Consistency\r\nacross partitioned nodes when failures happen in the cluster.\r\n\r\n\r\n### Amazon Dynamo\r\n\r\n\r\nAmazon Dynamo is a proprietary key-value store\r\ndeveloped by Amazon. It is designed for high performance, high\r\navailability, and continuous growth of data of huge volume. It is the\r\ndistributed, highly available, fault-tolerant skeleton for Amazon.\r\nDynamo is a peer-to-peer design meaning that each node is a peer and no\r\none is a master who manages the data.\r\n\r\nDynamo uses data replication and auto-sharding across multiple nodes of\r\nthe cluster. Imagine that a Dynamo cluster consists of many nodes. Every\r\nwrite operation in a node is replicated to two other nodes. Thus, there\r\nare three copies of data inside the cluster. If one of the nodes fails\r\nfor whatever reason, there are still two copies of data that can be\r\nretrieved. Auto-sharding ensures that the data is partitioned across the\r\ncluster.\r\n\r\n\r\n### Note\r\n\r\n**Auto-sharding**\r\n\r\nNoSQL database products usually support auto-sharding so that they can natively and automatically\r\ndistribute data across the database cluster. Data and workload are\r\nautomatically balanced across the nodes in the cluster. When a node\r\nfails for whatever reason, the failed node can be quickly and\r\ntransparently replaced without service interruptions.\r\n:::\r\n\r\nDynamo focuses primarily on the high availability of\r\na cluster and the most important idea is eventual consistency. While\r\nconsidering the CAP Theorem, Dynamo prefers Partition-tolerance and\r\nAvailability to Consistency. Dynamo introduces a mechanism called\r\n**Eventual Consistency** to support\r\nconsistency. Temporary inconsistency might occur in the cluster at a\r\npoint in time, but eventually all the nodes will receive the latest\r\nconsistent updates. Given a sufficiently long period of time without\r\nfurther changes, all the updates can be expected to propagate throughout\r\nthe cluster and the replicas on all the nodes will be consistent\r\neventually. In real life, an update takes only a fraction of a second to\r\nbecome eventually consistent. In other words, it is a trade-off between\r\nconsistency and latency.\r\n\r\n\r\n### Note\r\n\r\n**Eventual consistency**\r\n\r\nEventual consistency is not inconsistency. It is a weaker form of\r\nconsistency than the typical Atomic-Consistency-Isolation-Durability\r\n(ACID) type consistency is found in the relational\r\ndatabases. It implies that there can be short intervals of inconsistency\r\namong the replicated nodes during which the data gets updated among\r\nthese nodes. In other words, the replicas are updated asynchronously.\r\n\r\n\r\n\r\nCassandra's high-level architecture\r\n------------------------------------------------------\r\n\r\n\r\n\r\nCassandra runs on a peer-to-peer architecture which\r\nmeans that all nodes in the cluster have equal responsibilities except\r\nthat some of them are seed nodes for other non-seed nodes to obtain\r\ninformation about the cluster during startup. Each node holds a\r\npartition of the database. Cassandra provides automatic data\r\ndistribution and replication across all nodes in the cluster. Parameters\r\nare provided to customize the distribution and replication behaviors.\r\nOnce configured, these operations are processed in the background and\r\nare fully transparent to the application developers.\r\n\r\nCassandra is a column-family store and provides great schemaless\r\nflexibility to application developers. It is designed to manage huge\r\nvolume of data in a large cluster without a single point of failure. As\r\nmultiple copies of the same data (replicas) are replicated in the\r\ncluster, whenever one node fails for whatever reason, the other replicas\r\nare still available. Replication can be configured to meet the different\r\nphysical cluster settings, including data center and rack locations.\r\n\r\nAny node in the cluster can accept read or write requests from a client.\r\nThe node that is connected to a client with a request serves as the\r\ncoordinator of that particular request. The coordinator determines which\r\nnodes are responsible for holding the data for the request and acts as a\r\nproxy between the client and the nodes.\r\n\r\nCassandra borrows the commitlog mechanism from Google BigTable to ensure\r\ndata durability. Whenever a write data request is received by a node, it\r\nis written into the commitlog. The data that is being updated is then\r\nwritten to a memory structure, known as memtable. When the memtable is\r\nfull, the data inside the memtable is flushed to a disk storage\r\nstructure, SSTable. The writes are automatically partitioned by the row\r\nkey and replicated to the other nodes holding the same partition.\r\n\r\nCassandra provides linear scalability, which means that the performance\r\nand capacity of the cluster is proportional to the number of nodes in\r\nit.\r\n\r\n\r\n\r\n### Partitioning\r\n\r\n\r\nThe ability to scale horizontally and incrementally\r\nis a Cassandra key design feature. To achieve this, Cassandra is required to dynamically partition\r\nthe data over the set of nodes in the cluster.\r\n\r\nA cluster is the outermost structure which is\r\ncomposed of nodes in Cassandra. It is also a container of keyspace. A\r\nkeyspace in Cassandra is analogous to a schema in a\r\nrelational database. Each Cassandra cluster has a system keyspace to\r\nkeep system-wide metadata. It contains the replication settings which\r\ncontrols how the data is distributed and replicated in a cluster.\r\nTypically, one keyspace is assigned to one cluster but one cluster might\r\ncontain more than one keyspace.\r\n\r\nThe smallest cluster in the theory contains a single node and a cluster\r\nof three or more nodes, which is much more practical. Each node holds a\r\nreplica for the different range of data in partitions, and exchanges\r\ninformation across the cluster every second.\r\n\r\nA client issues read or write requests to any node. The node that\r\nreceives the request becomes a coordinator that acts as a proxy of the\r\nclient to do the things as explained previously. Data is distributed\r\nacross the cluster and the node addressing mechanism is called\r\nconsistent hashing. Therefore, a cluster can be viewed as a ring of hash\r\nas each node in the cluster or the ring is assigned a single unique\r\ntoken so that each node is responsible for the data in the range from\r\nits assigned token to that of the previous node. For example, in the\r\nfollowing figure, a cluster contains four nodes with unique tokens:\r\n\r\n\r\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_02.jpg)\r\n\r\n:::\r\nCassandra's consistent hashing\r\n:::\r\n\r\nBefore Version 1.2, tokens were calculated and\r\nassigned manually and from Version 1.2 onwards,\r\ntokens can be generated automatically. Each row has a row key used by a\r\npartitioner to calculate its hash value. The hash\r\nvalue determines the node which stores the first replica of the row. The\r\npartitioner is just a hash function that is used for calculating a row\r\nkey's hash value and it also affects how the data is distributed or\r\nbalanced in the cluster. When a write occurs, the first replica of the\r\nrow is always placed in the node with the key range of the token. For\r\nexample, the hash value of a row key `ORACLE` is\r\n`6DE7` that falls in the range of 4,000 and 8,000 and so the\r\nrow goes to the bottom node first. All the remaining replicas are\r\ndistributed based on the replication strategy.\r\n\r\n\r\n### Note\r\n\r\n**Consistent hashing**\r\n\r\nConsistent hashing allows each node in the cluster\r\nto independently determine which nodes are replicas for a given row key.\r\nIt just involves hashing the row key, and then compares that hash value\r\nto the token of each node in the cluster. If the hash value falls in\r\nbetween a node's token, and the token of the previous node in the ring\r\n(tokens are assigned to nodes in a clockwise direction), that node is\r\nthe replica for that row.\r\n:::\r\n\r\n\r\n### Replication\r\n\r\n\r\nCassandra uses replication to attain high availability and data durability. Each data is replicated at\r\na number of nodes that are configured by a parameter called replication\r\nfactor. The coordinator commands the replication of the data within its\r\nrange. It replicates the data to the other nodes in the ring. Cassandra\r\nprovides the client with various configurable options to see how the\r\ndata is to be replicated, which is called replication strategy.\r\n\r\nReplication strategy is the method of determining which nodes the\r\nreplicas are placed in. It provides many options, such as rack-aware,\r\nrack-unaware, network-topology-aware, so on and so forth.\r\n\r\n\r\n### Snitch\r\n\r\n\r\nA snitch determines which data centers and racks to\r\ngo for in order to make Cassandra aware of the\r\nnetwork topology for routing the  requests\r\nefficiently. It affects how the replicas can be distributed while\r\nconsidering the physical setting of the data centers and racks. The node\r\nlocation can be determined by the rack and data center with reference to\r\nthe node's IP address. An example of a cluster across two data centers\r\nis shown in the following figure, in order to illustrate the\r\nrelationship among replication factor, replication strategy, and snitch\r\nin a better way:\r\n\r\n\r\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_03.jpg)\r\n\r\n:::\r\nMultiple data center cluster\r\n:::\r\n\r\nEach data center has two racks and each rack contains two nodes\r\nrespectively. The replication factor per data center is set to three\r\nhere. With two data centers, there are six replicas in total. The node\r\nlocation that addresses the data center and rack locations are subject\r\nto the convention of IP address assignment of the nodes.\r\n\r\n\r\n### Seed node\r\n\r\n\r\nSome nodes in a Cassandra cluster are designated as seed nodes for the others. They are\r\nconfigured to be the first nodes to start in the cluster. They also\r\nfacilitate the bootstrapping process for the new nodes joining the\r\ncluster. When a new node comes online, it will talk to the seed node to\r\nobtain information about the other nodes in the cluster. The talking\r\nmechanism is called  **gossip**. If a\r\ncluster is across multiple data centers, the best practice is to have\r\nmore than one seed node per data center.\r\n\r\n\r\n### Gossip and Failure detection\r\n\r\n\r\nNodes need to communicate periodically (every\r\nsecond) to exchange state information (for example,\r\ndead or alive), about themselves and about other nodes they know about.\r\nCassandra uses a gossip communication protocol to disseminate the state\r\ninformation, which is  also known as epidemic protocol. It is a peer-to-peer communication\r\nprotocol that provides a decentralized, periodic,\r\nand an automatic way for the nodes in the cluster to exchange the state\r\ninformation about themselves, and about other nodes they know about with\r\nup to three other nodes. Therefore, all nodes can quickly learn about\r\nall the other nodes in the cluster. Gossip information is also persisted\r\nlocally by each node to allow fast restart.\r\n\r\nCassandra uses a very efficient algorithm, called *Phi Accrual Failure\r\nDetection Algorithm*, to detect the\r\nfailure of a node. The idea of the algorithm is that the failure\r\ndetection is not represented by a Boolean value stating whether a node\r\nis up or down. Instead, the algorithm outputs a value on the continuous\r\nsuspicion level between dead and alive, on how confident it is that the\r\nnode has failed. In a distributed environment, false negatives might\r\nhappen due to the network performance, fluctuating workload, and other\r\nconditions. The algorithm takes all these factors into account and\r\nprovides a probabilistic value. If a node has failed, the other nodes\r\nperiodically try to gossip with it to see if it comes back online. A\r\nnode can then determine locally from the gossip state and its history\r\nand adjust routes accordingly.\r\n\r\n\r\n### Write path\r\n\r\n\r\nThe following figure depicts the  components and\r\ntheir sequence of executions that form a write path:\r\n\r\n\r\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_04.jpg)\r\n\r\n:::\r\nCassandra write path\r\n:::\r\n\r\nWhen a write occurs, the data will be immediately appended to the\r\ncommitlog on the disk to ensure write durability. Then Cassandra stores\r\nthe data in memtable, an in-memory store of hot and fresh data. When\r\nmemtable is full, the memtable data will be flushed to a disk file,\r\ncalled SSTable, using sequential I/O and so random I/O is avoided. This\r\nis the reason why the write performance is so high. The commitlog is\r\npurged after the flush.\r\n\r\nDue to the intentional adoption of sequential I/O, a row is typically\r\nstored across many SSTable files. Apart from its data, SSTable also has\r\na primary index and a *bloom filter*. A primary index is a\r\nlist of row keys and the start position of rows in the data file.\r\n\r\n\r\n### Note\r\n\r\n**Bloom filter**\r\n\r\nBloom filter is a sample subset of the primary index\r\nwith very fast nondeterministic algorithms to check whether an element\r\nis a member of a set. It is used to boost the performance.\r\n:::\r\n\r\nFor write operations, Cassandra supports tunable\r\nconsistency by various write consistency levels. The write consistency\r\nlevel is the number of replicas that acknowledge a successful write. It\r\nis tunable on a spectrum of write consistency levels, as shown in the\r\nfollowing figure:\r\n\r\n\r\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_05.jpg)\r\n\r\n:::\r\nCassandra write consistency levels\r\n:::\r\n\r\nThe following describes the terms in the figure:\r\n\r\n\r\n-   **ANY**: This is the lowest\r\n    consistency (but highest availability)\r\n\r\n-   **ALL**: This is  the highest\r\n    consistency (but lowest availability)\r\n\r\n-   **ONE**: This gives at least one\r\n    replica\r\n\r\n-   **TWO**: This gives at least two\r\n    replicas\r\n\r\n-   **THREE**: This gives at least three\r\n    replicas\r\n\r\n-   **QUORUM**: This ensures strong\r\n    consistency by tolerating some level of failure, which is determined\r\n    by *(replication\\_factor / 2) + 1* (rounded down to the\r\n    nearest integer)\r\n\r\n-   **LOCAL\\_QUORUM**: This is  for\r\n    multi-data center and rack-aware without inter-data center traffic\r\n\r\n-   **EACH\\_QUORUM**: This is for\r\n    multi-data center and rack-aware\r\n:::\r\n\r\nThe two extremes are the leftmost **ANY**\r\nwhich means weak consistency and the rightmost **ALL** means\r\nstrong consistency. A consistency level of\r\n**THREE** is very common in practice. **QUORUM**\r\ncan be chosen to be an optimum value, as calculated by the given\r\nformula. Here, the replication factor is the number of replicas of data\r\non multiple nodes. Both **LOCAL QUORUM** and **EACH\r\nQUORUM** support multiple data centers and rack-aware write\r\nconsistency with a slight difference as shown earlier.\r\n\r\n\r\n### Read path\r\n\r\n\r\nOn the flip side, the following figure shows the\r\ncomponents and their sequence of executions that\r\nform a read path:\r\n\r\n\r\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_06.jpg)\r\n\r\n:::\r\nCassandra read path\r\n:::\r\n\r\nWhen a read request comes in to a node, the data to\r\nbe returned is merged from all the related SSTables and any unflushed\r\nmemtables. Timestamps are used to determine which one is up-to-date. The\r\nmerged value is also stored in a write-through row cache to improve the\r\nfuture read performance.\r\n\r\nSimilar to the write consistency levels, Cassandra also provides tunable\r\nread consistency levels, as shown in the following figure:\r\n\r\n\r\n![](https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_07.jpg)\r\n\r\n:::\r\nCassandra read consistency levels\r\n:::\r\n\r\nThe following describes the terms in the figure:\r\n\r\n\r\n-   **ALL**: This is  the highest\r\n    consistency (but lowest availability)\r\n\r\n-   **ONE**: This gives at least one\r\n    replica\r\n\r\n-   **TWO**: This gives at least two\r\n    replicas\r\n\r\n-   **THREE**: This  gives at least three\r\n    replicas\r\n\r\n-   **QUORUM**: This ensures strong\r\n    consistency by tolerating some level of failure, which is determined\r\n    by *(replication\\_factor / 2) + 1* (rounded down to the\r\n    nearest integer)\r\n\r\n-   **LOCAL\\_QUORUM**: This is  for\r\n    multi-data center and rack-aware without inter-data center traffic\r\n\r\n-   **EACH\\_QUORUM**: This is for\r\n    multi-data center and rack-aware\r\n:::\r\n\r\nRead consistency level is the number of replicas contacted for a\r\nsuccessful, consistent read, almost identical to write consistency\r\nlevels, except that **ANY** is not an option here.\r\n\r\n\r\n### Repair mechanism\r\n\r\n\r\nThere are three built-in repair mechanisms provided\r\nby Cassandra:\r\n\r\n\r\n-   Read repair\r\n\r\n-   Hinted handoff\r\n\r\n-   Anti-entropy node repair\r\n:::\r\n\r\nDuring a read, the coordinator that is just the node connects and\r\nservices the client, contacts a number of nodes as specified by the\r\nconsistency level for data and the fastest replicas will return the data\r\nfor a consistency check by in-memory comparison. As it is not a\r\ndedicated node, Cassandra lacks a single point of failure. It also\r\nchecks all the remaining replicas in the background. If a replica is\r\nfound to be inconsistent, the coordinator will issue an update to bring\r\nback the consistency. This mechanism is called \r\n**read repair**.\r\n\r\n**Hinted handoff** aims at reducing the\r\ntime to restore a failed node when rejoining the cluster. It ensures\r\nabsolute write availability by sacrificing a bit of\r\nread consistency. If a replica is down at the time a write occurs,\r\nanother healthy replica stores a hint. Even worse, if all the relevant\r\nreplicas are down, the coordinator stores the hint locally. The hint\r\nbasically contains the location of the failed replica, the affected row\r\nkey, and the actual data that is being written. When a node responsible\r\nfor the token range is up again, the hint will be handed off to resume\r\nthe write. As such, the update cannot be read before a complete handoff,\r\nleading to inconsistent reads.\r\n\r\nAnother repair mechanism is called \r\n**anti-entropy** which is a replica synchronization mechanism\r\nto ensure up-to-date data on all nodes and is run by the administrators\r\nmanually.\r\n\r\n\r\n\r\nFeatures of Cassandra\r\n---------------------------------------\r\n\r\n\r\n\r\nIn order to keep this lab short, the following\r\nbullet list covers the great features provided by Cassandra:\r\n\r\n\r\n-   Written in Java and hence providing native Java support\r\n\r\n-   Blend of Google BigTable and Amazon Dynamo\r\n\r\n-   Flexible schemaless column-family data model\r\n\r\n-   Support for structured and unstructured data\r\n\r\n-   Decentralized, distributed peer-to-peer architecture\r\n\r\n-   Multi-data center and rack-aware data replication\r\n\r\n-   Location transparent\r\n\r\n-   Cloud enabled\r\n\r\n-   Fault-tolerant with no single point of failure\r\n\r\n-   An automatic and transparent failover\r\n\r\n-   Elastic, massively, and linearly scalable\r\n\r\n-   Online node addition or removal\r\n\r\n-   High Performance\r\n\r\n-   Built-in data compression\r\n\r\n-   Built-in caching layer\r\n\r\n-   Write-optimized\r\n\r\n-   Tunable consistency providing choices from very strong consistency\r\n    to different levels of eventual consistency\r\n\r\n-   Provision of **Cassandra Query Language**\r\n    (**CQL**), a SQL-like language\r\n    imitating `INSERT` , `UPDATE` ,\r\n    `DELETE` , `SELECT` syntax of SQL\r\n\r\n-   Open source and community-driven\r\n\r\n\r\n\r\nSummary\r\n-------------------------\r\n\r\n\r\n\r\nIn this lab, we have gone through a bit of history starting from the\r\n1970s. We were in total control of the data models that were rather\r\nstable and the applications that were pretty simple. The relational\r\ndatabases were a perfect fit in the old days. With the emergence of\r\nobject-oriented programming and the explosion of the web applications on\r\nthe pervasive Internet, the nature of the data has been extended from\r\nstructured to semi-structured and unstructured. Also, the application\r\nhas become more complex. The relational databases could not be perfect\r\nagain. The concept of Big Data was created to describe such challenges\r\nand NoSQL databases provide an alternative resolution to the relational\r\ndatabases.\r\n\r\nNoSQL databases are of a wide variety. They provide some common benefits\r\nand can be classified by the NoSQL database type. Apache Cassandra is\r\none of the NoSQL databases that is a blend of Google BigTable and Amazon\r\nDynamo. The elegance of its architecture inherits from the DNA of these\r\ntwo parents.\r\n\r\nIn the next lab, we will look at the flexible data model supported\r\nby Cassandra.\r\n",
      "user": "anonymous",
      "dateUpdated": "2020-06-07T07:42:44+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "text",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/text",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Lab 1. Bird&rsquo;s Eye View of Cassandra</h2>\n<p>Imagine that we have turned back the clock to the 1990s and you an<br />\napplication architect. Whenever you were required to select a suitable<br />\ndatabase technology for your applications, what kind of database<br />\ntechnology would you choose? I bet 95 percent (or more) of the time you<br />\nwould select relational databases.</p>\n<p><strong>Relational databases</strong> have been the<br />\nmost dominating data management solution since the 1970s. At that time,<br />\nthe application system was usually silo. The users of the application<br />\nand their usage patterns were known and under control. The workload that<br />\nhad to be catered for by the relational database could be determined and<br />\nestimated. Apart from the workload consideration, the data model can<br />\nalso be structured in normalized forms as recommended by the relational<br />\ntheory. Moreover, relational databases provide many benefits such as<br />\nsupport of transactions, data consistency, and isolation. Relational<br />\ndatabases just fit perfectly for the purposes. Therefore, it is not<br />\ndifficult to understand why the relational database has been so popular<br />\nand why it is the de facto standard for persistent data stores in<br />\napplication development.</p>\n<p>Nonetheless, with the proliferation of the Internet and the numerous web<br />\napplications running on it, the control of the users and their usage<br />\npatterns (hence the scale), the workload generated, and the flexibility<br />\nof the data model were gone. Typical examples of these web applications<br />\nwere global e-commerce websites, social media sites, video community<br />\nwebsites, and so on. They generated a tremendous amount of data in a<br />\nvery short period of time. It should also be noted that the data<br />\ngenerated by these applications were not only structured, but also<br />\nsemi-structured and even unstructured. Since relational databases were<br />\nthe de facto standard at that time, developers and architects did not<br />\nhave many alternatives but were forced to tweak them to support these<br />\nweb applications, even though they knew that relational databases were<br />\nsuboptimal and had many limitations. It became apparent that a different<br />\nkind of enabling technology should be found to break through the<br />\nchallenges.</p>\n<p>We are in an era of information explosion, as a result of the<br />\never-increasing amount of user-generated data and content on the Web and<br />\nmobile applications. The generated data is not only large in volume and<br />\nfast in velocity but it is also diversified in variety. Such rapidly<br />\ngrowing data of different varieties is often termed as <strong>Big Data</strong>.</p>\n<p>No one has a clear, formal definition of Big Data. People, however,<br />\nunanimously agree that the most fundamental characteristics of Big Data<br />\nare related to large volume, high velocity, and great variety. Big Data<br />\nimposes real, new challenges to the information systems that have<br />\nadopted traditional ways of handling data. These systems are not<br />\ndesigned for web-scale and for being enhanced to do so, cost<br />\neffectively. Due to this, you might find yourself asking whether or not<br />\nwe have any alternatives.</p>\n<p>Challenges come with opportunities on the flip side. A new breed of data<br />\nmanagement products was born. The most recent answer to the question in<br />\nthe last paragraph is NoSQL.</p>\n<h2>What is NoSQL?</h2>\n<p>The need to tackle the Big Data challenges has led to<br />\nthe emergence of new data management technologies and techniques. Such<br />\ntechnologies and techniques are rather different from the ubiquitous<br />\nrelational database technology that has been used for over 40 years.<br />\nThey are collectively known as <strong>NoSQL</strong>.</p>\n<p>NoSQL is an umbrella term for the data stores that are not based on the<br />\nrelational data model. It encompasses a great variety of many different<br />\ndatabase technologies and products. As shown in the following figure,<br />\nThe <strong>Data Platforms Landscape Map</strong>,<br />\nthere are over 150 different database products that belong to the<br />\nnon-relational school as mentioned in<br />\n<a href=\"http://nosql-database.org/\">http://nosql-database.org/</a>. Cassandra is one of the most popular ones.<br />\nOther popular NoSQL database products are, just to name a few, MongoDB,<br />\nRiak, Redis, Neo4j, so on and so forth.</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_01.jpg\" alt=\"\" /></p>\n<p>:::<br />\nThe Data Platforms Landscape Map (Source: 451 Research)<br />\n:::</p>\n<p>So, what kinds of benefits are provided by NoSQL? When compared to the<br />\nrelational database, NoSQL overcomes the weaknesses that the relational<br />\ndata model does not address well, which are as<br />\nfollows:</p>\n<ul>\n<li>\n<p>Huge volume of structured, semi-structured, and unstructured data</p>\n</li>\n<li>\n<p>Flexible data model (schema) that is easy to change</p>\n</li>\n<li>\n<p>Scalability and performance for web-scale applications</p>\n</li>\n<li>\n<p>Lower cost</p>\n</li>\n<li>\n<p>Impedance mismatch between the relational data model and<br />\nobject-oriented programming</p>\n</li>\n<li>\n<p>Built-in replication</p>\n</li>\n<li>\n<p>Support for agile software<br />\ndevelopment</p>\n</li>\n</ul>\n<h3>Note</h3>\n<p><strong>Limitations of NoSQL Databases</strong></p>\n<p>Many NoSQL databases do not support transactions.<br />\nThey use replication extensively so that the data in the cluster might<br />\nbe momentarily inconsistent (although it is eventually consistent). In<br />\naddition, the range queries are not available in NoSQL databases.<br />\nFurthermore, a flexible schema might lead to problems with efficient<br />\nsearches.<br />\n:::</p>\n<p>The huge volume of structured, semi-structured, and unstructured data<br />\nwas mentioned earlier. What I want to dive deeper into here is that<br />\ndifferent NoSQL databases provide different solutions for each of them.<br />\nThe primary factor to be considered is the NoSQL database type, which<br />\nwill be introduced in the subsequent section.</p>\n<p>All NoSQL databases provide a flexible data model<br />\nthat is easy to change and some might be even schemaless. In a<br />\nrelational database, the relational data model is called schema. You<br />\nneed to understand the data to be stored in a relational database,<br />\ndesign the data model according to the relational database theory, and<br />\ndefine the schema upfront in the relational database before you can<br />\nactually store data inside it. It is a very structured approach for<br />\nstructured data. It is a prescriptive data modeling process. It is<br />\nabsolutely fine if the data model is stable, because there are not many<br />\nchanges required. But what if the data model keeps changing in the<br />\nfuture and you do not know what needs to be changed? You cannot<br />\nprescribe comprehensively in advance. It leads to many inevitable<br />\nremedies; say, data patching for example, to change the schema.</p>\n<p>Conversely, in NoSQL databases, you need not prescribe comprehensively.<br />\nYou only need to describe what is to be stored. You are not bound by the<br />\nrelational database theory. You are allowed to change the data model<br />\nwhenever necessary. The data model is schemaless and is a living object.<br />\nIt evolves as life goes on. It is a descriptive data modeling process.</p>\n<p>Scalability and performance for web-scale applications refer to the<br />\nability of the system to be scaled, preferably horizontally, to support<br />\nweb-scale workloads without considerably deteriorating system<br />\nperformance. Relational databases can only be scaled out to form a<br />\ncluster consisting of a very small number of nodes. It implies the<br />\nrather low ceiling imposed on these web-scale applications using<br />\nrelational databases. In addition, changing the schema in a clustered<br />\nrelational database is a big task of high complexity. The processing<br />\npower required to do this is so significant that the system performance<br />\ncannot be unaffected. Most NoSQL databases were created to serve<br />\nweb-scale applications. They natively support horizontal scaling without<br />\nvery little degrade on the performance.</p>\n<p>Now let us talk about money. Traditionally, most<br />\nhigh-end relational databases are commercial products that demand their<br />\nusers to pay huge software license fees. Besides, to run these high-end<br />\nrelational databases, the underlying hardware servers are usually<br />\nhigh-end as well. The result is that the hardware and software costs of<br />\nrunning a powerful relational database are exceptionally large. In<br />\ncontrast, NoSQL databases are open source and community-driven in a<br />\nmajority, meaning that you need to pay the software license cost, which<br />\nis an order of magnitude less than other databases. NoSQL databases are<br />\nable to run on commodity machines that will lead to a possible churn, or<br />\ncrashes. Therefore, the machines are usually configured to be a cluster.<br />\nHigh-end hardware servers are not needed and so the hardware cost is<br />\ntremendously reduced. It should be noted that when NoSQL databases are<br />\nput into production, some cost of the support is still required but it<br />\nis definitely much less when compared to that of commercial products.</p>\n<p>There exists a generation gap between the relational data model and<br />\nobject-oriented programming. The relational data model was the product<br />\nof 1970s, whereas object-oriented programming became very popular in<br />\n1990s. The root cause, known as impedance mismatch, is an inherent<br />\ndifficulty of representing a record or a table in a relational data<br />\nmodel with the object-oriented model. Although there are resolutions for<br />\nthis difficulty, most application developers still feel very frustrated<br />\nto bring the two together.</p>\n<h3>Note</h3>\n<p><strong>Impedance Mismatch</strong></p>\n<p>Impedance mismatch is the difference between the<br />\nrelational model and the in-memory data structures that are usually<br />\nencountered in object-oriented programming languages.<br />\n:::</p>\n<p>Built-in replication is a feature that most NoSQL databases provide to<br />\nsupport high availability in a cluster of many nodes. It is usually<br />\nautomatic and transparent to the application developers. Such a feature<br />\nis also available in relational databases, but the database<br />\nadministrators must struggle to configure, manage, and operate it by<br />\nthemselves.</p>\n<p>Finally, relational databases do not support agile software development<br />\nvery well. Agile software development is iterative by nature. The<br />\nsoftware architecture and data model emerge and evolve as the project<br />\nproceeds in order to deliver the product incrementally. Hence, it is<br />\nconceivable that the need of changing the data model to meet the new<br />\nrequirements is inevitably frequent. Relational databases are structured<br />\nand do not like changes. NoSQL can provide such flexibility for agile<br />\nsoftware development teams by virtue of its schemaless characteristic.<br />\nEven better, NoSQL databases usually allow the changes to be implemented<br />\nin real time without any downtime.</p>\n<h3>NoSQL Database types</h3>\n<p>Now you know the benefits of NoSQL databases, but<br />\nthe products that fall under the NoSQL databases umbrella are quite<br />\nvaried. How can you select the right one for yourself among so many<br />\nNoSQL databases? The selection criteria of which NoSQL database fits<br />\nyour needs is really dependent on the use cases at hand. The most<br />\nimportant factor to consider here is the NoSQL database type, which can<br />\nbe subdivided into four main categories:</p>\n<ul>\n<li>\n<p>Key/value pair store</p>\n</li>\n<li>\n<p>Column-family store</p>\n</li>\n<li>\n<p>Document-based repository</p>\n</li>\n<li>\n<p>Graph database<br />\n:::</p>\n</li>\n</ul>\n<p>The NoSQL database type dictates the data model that you can use. It is<br />\nbeneficial to understand each of them deeper.</p>\n<h4>Key/value pair store</h4>\n<p>Key/value pair is the simplest NoSQL database type.<br />\nKey/value store is similar to the concept of Windows registry, or in<br />\nJava or C#, a map, a hash, a key/value pair. Each<br />\ndata item is represented as an attribute name, also a key, together with<br />\nits value. It is also the basic unit stored in the database. Examples of<br />\nthe NoSQL databases of key/value pair type<br />\nare <strong>Amazon Dynamo</strong>, <strong>Berkeley<br />\nDB</strong>, <strong>Voldemort</strong> and<br />\n<strong>Riak</strong>.</p>\n<p>Internally, key/value pairs are stored in a data structure called <strong>hashmap</strong>. Hashmap is popular because it<br />\nprovides very good performance on accessing data. The key of a key/value<br />\npair is unique and can be searched very quickly.</p>\n<p>Key/value pair can be stored and distributed in the disk storage as well<br />\nas in memory. When used in memory, it can be used as a cache, which<br />\ndepends on the caching algorithm, can considerably reduce disk I/O and<br />\nhence boost up the performance significantly.</p>\n<p>On the flip side, key/value pair has some drawbacks, such as lack of<br />\nsupport of range queries, no way to operate on multiple keys<br />\nsimultaneously, and possible issues with load balancing.</p>\n<h4>Column-family store</h4>\n<p>A column in this context is not equal to a column in<br />\na relational table. In the NoSQL world, a column is a data structure<br />\nthat contains a key, value, and timestamp. Thus, it can be regarded as a<br />\ncombination of key/value pair and a timestamp.<br />\nExamples are <strong>Google BigTable</strong>,<br />\n<strong>Apache Cassandra</strong>, and  <strong>Apache<br />\nHBase</strong>. They provide optimized performance for queries over<br />\nvery large datasets.</p>\n<p>Column-family store is basically a multi-dimensional<br />\nmap. It stores columns of data together as a row, which is associated<br />\nwith a row key. This contrasts with rows of data in a relational<br />\ndatabase. Column-family store does not need to store null columns, as in<br />\nthe case of a relational database and so it consumes much less disk<br />\nspace. Moreover, columns are not bound by a rigid schema and you are not<br />\nrequired to define the schema upfront.</p>\n<p>The key component of a column is usually called the primary key or the<br />\nrow key. Columns are stored in a sorted manner by the row key. All the<br />\ndata belonging to a row key is stored together. As such, read and write<br />\noperations of the data can be confined to a local node, avoiding<br />\nunnecessary inter-node network traffic in a cluster. This mechanism<br />\nmakes the data lookup and retrieval extremely efficient.</p>\n<p>Obviously, a column-family store is not the best solution for systems<br />\nthat require ACID transactions and it lacks the support for aggregate<br />\nqueries provided by relational databases such as <code>SUM()</code>.</p>\n<h4>Document-based repository</h4>\n<p>Document-based repository is designed for documents<br />\nor semi-structured data. The basic unit of a<br />\ndocument-based repository associates each key, a primary identifier,<br />\nwith a complex data structure called a document. A document can contain<br />\nmany different key-value pairs, or key-array pairs, or even nested<br />\ndocuments. Therefore, document-based repository does not adhere to a<br />\nschema. Examples are <strong>MongoDB</strong> and<br />\n<strong>CouchDB</strong>.</p>\n<p>In practice, a document is usually a loosely structured set of key/value<br />\npairs in the form of  <strong>JavaScript Object<br />\nNotation</strong> (<strong>JSON</strong>). Document-based repository<br />\nmanages a document as a whole and avoids breaking up a document into<br />\nfragments of key/value pairs. It also allows document properties to be<br />\nassociated with a document.</p>\n<p>As a document does not<br />\nadhere to a fixed schema, the search performance is not guaranteed.<br />\nThere are generally two approaches to query a document database. The<br />\nfirst is to use materialized views (such as CouchDB) that are prepared<br />\nin advance. The second is to use indexes defined on the document values<br />\n(such as MongoDB) that behave in the same way as a relational database<br />\nindex.</p>\n<h4>Graph database</h4>\n<p>Graph databases are designed for storing information<br />\nabout networks, such as a social network. A graph is<br />\nused to represent the highly connected network that is composed of nodes<br />\nand their relationships. The nodes and relationships can have individual<br />\nproperties. The prominent graph databases include<br />\n<strong>Neo4J</strong> and  <strong>FlockDB</strong>.</p>\n<p>Owing to the unique characteristics of a graph, graph databases commonly<br />\nprovide APIs for rapid traversal of graphs.</p>\n<p>Graph databases are particularly difficult to be scaled out with<br />\nsharding because traversing a graph of the nodes on different machine<br />\ndoes not provide a very good performance. It is also not a<br />\nstraightforward operation to update all or a subset of the nodes at the<br />\nsame time.</p>\n<p>So far, you have grasped the fundamentals of the NoSQL family. Since<br />\nthis course concentrates on Apache Cassandra and its data model, you need<br />\nto know what Cassandra is and have a basic understanding of what its<br />\narchitecture is, so that you can select and leverage the best available<br />\noptions when you are designing your NoSQL data model and application.</p>\n<h2>What is Cassandra?</h2>\n<p><strong>Cassandra</strong> can be simply described in<br />\na single phrase: a massively scalable, highly available open source<br />\nNoSQL database that is based on peer-to-peer architecture.</p>\n<p>Cassandra is now 5 years old. It is an active open source project in the<br />\nApache Software Foundation and therefore it is known as Apache Cassandra<br />\nas well. Cassandra can manage huge volume of structured,<br />\nsemi-structured, and unstructured data in a large distributed cluster<br />\nacross multiple data centers. It provides linear scalability, high<br />\nperformance, fault tolerance, and supports a very flexible data model.</p>\n<h3>Note</h3>\n<p><strong>Netflix and Cassandra</strong></p>\n<p>One very famous case study of Cassandra is<br />\nNetflix&rsquo;s move to replace their Oracle SQL database to Cassandra<br />\nrunning on cloud. As of March 2013, Netflix&rsquo;s Cassandra deployment<br />\nconsists of 50 clusters with over 750 nodes. For more information,<br />\nplease visit the case study at<br />\n<a href=\"http://www.datastax.com/wp-content/uploads/2011/09/CS-Netflix.pdf\">http://www.datastax.com/wp-content/uploads/2011/09/CS-Netflix.pdf</a>.<br />\n:::</p>\n<p>In fact, many of the benefits that Cassandra provides are inherited from<br />\nits two best-of-breed NoSQL parents, Google BigTable and Amazon Dynamo.<br />\nBefore we go into the details of Cassandra&rsquo;s architecture, let us walk<br />\nthrough each of them first.</p>\n<h3>Google BigTable</h3>\n<p>Google BigTable is Google&rsquo;s core technology,<br />\nparticularly addressing data persistence and management on web-scale. It<br />\nruns the data stores for many Google applications, such as Gmail,<br />\nYouTube, and Google Analytics. It was designed to be a web-scale data<br />\nstore without sacrificing real-time responses. It has superb read and<br />\nwrite performance, linear scalability, and continuous availability.</p>\n<p>Google BigTable is a sparse, distributed, persistent, multidimensional<br />\nsorted map. The map is indexed by a row key.</p>\n<p>Despite the many benefits Google BigTable provides, the underlying<br />\ndesign concept is really simple and elegant. It uses a persistent<br />\ncommitlog for every data write request that it receives and then writes<br />\nthe data into a memory store (acting as a cache). At regular intervals<br />\nor when triggered by a particular event, the memory store is flushed to<br />\npersistent disk storage by a background process. This persistent disk<br />\nstorage is called <strong>Sorted String Table</strong>, or<br />\n<strong>SSTable</strong>. The SSTable is immutable<br />\nmeaning that once it has been written to a disk, it will never be<br />\nchanged again. The word <em>sorted</em> means that the data inside<br />\nthe SSTable is indexed and sorted and hence the data can be found very<br />\nquickly. Since the write operation is log-based and memory-based, it<br />\ndoes not involve any read operation, and therefore the write operation<br />\ncan be extremely fast. If a failure happens, the commitlog can be used<br />\nto replay the sequence of the write operations to merge the data that<br />\npersists in the SSTables.</p>\n<p>Read operation is also very efficient by looking up the data in the<br />\nmemory store and the indexed SSTables, which are then merged to return<br />\nthe data.</p>\n<p>All the above-mentioned Google BigTable brilliances<br />\ndo come with a price. Because Google BigTable is distributed in nature,<br />\nit is constrained by the famous <em>CAP theorem</em>, stating the<br />\nrelationship among the three characteristics of a distributed system,<br />\nnamely Consistency, Availability, and Partition-tolerance. In a<br />\nnutshell, Google BigTable prefers Consistency and Partition-tolerance to<br />\nAvailability.</p>\n<h3>Note</h3>\n<p><strong>The CAP theorem</strong></p>\n<p>CAP is an acronym of the  three characteristics of a<br />\ndistributed system: Consistency, Availability, and Partition-tolerance.<br />\nConsistency means that all the nodes in a cluster see the same data at<br />\nany point in time. Availability means that every request that is<br />\nreceived by a non-failing node in the cluster must result in a response.<br />\nPartition-tolerance means that a node can still function when<br />\ncommunication with other groups of nodes is lost. Originating from Eric<br />\nA. Brewer, the theorem states that in a distributed system, only two out<br />\nof the three characteristics can be attained at the most.<br />\n:::</p>\n<p>Google BigTable has trouble with Availability while keeping Consistency<br />\nacross partitioned nodes when failures happen in the cluster.</p>\n<h3>Amazon Dynamo</h3>\n<p>Amazon Dynamo is a proprietary key-value store<br />\ndeveloped by Amazon. It is designed for high performance, high<br />\navailability, and continuous growth of data of huge volume. It is the<br />\ndistributed, highly available, fault-tolerant skeleton for Amazon.<br />\nDynamo is a peer-to-peer design meaning that each node is a peer and no<br />\none is a master who manages the data.</p>\n<p>Dynamo uses data replication and auto-sharding across multiple nodes of<br />\nthe cluster. Imagine that a Dynamo cluster consists of many nodes. Every<br />\nwrite operation in a node is replicated to two other nodes. Thus, there<br />\nare three copies of data inside the cluster. If one of the nodes fails<br />\nfor whatever reason, there are still two copies of data that can be<br />\nretrieved. Auto-sharding ensures that the data is partitioned across the<br />\ncluster.</p>\n<h3>Note</h3>\n<p><strong>Auto-sharding</strong></p>\n<p>NoSQL database products usually support auto-sharding so that they can natively and automatically<br />\ndistribute data across the database cluster. Data and workload are<br />\nautomatically balanced across the nodes in the cluster. When a node<br />\nfails for whatever reason, the failed node can be quickly and<br />\ntransparently replaced without service interruptions.<br />\n:::</p>\n<p>Dynamo focuses primarily on the high availability of<br />\na cluster and the most important idea is eventual consistency. While<br />\nconsidering the CAP Theorem, Dynamo prefers Partition-tolerance and<br />\nAvailability to Consistency. Dynamo introduces a mechanism called<br />\n<strong>Eventual Consistency</strong> to support<br />\nconsistency. Temporary inconsistency might occur in the cluster at a<br />\npoint in time, but eventually all the nodes will receive the latest<br />\nconsistent updates. Given a sufficiently long period of time without<br />\nfurther changes, all the updates can be expected to propagate throughout<br />\nthe cluster and the replicas on all the nodes will be consistent<br />\neventually. In real life, an update takes only a fraction of a second to<br />\nbecome eventually consistent. In other words, it is a trade-off between<br />\nconsistency and latency.</p>\n<h3>Note</h3>\n<p><strong>Eventual consistency</strong></p>\n<p>Eventual consistency is not inconsistency. It is a weaker form of<br />\nconsistency than the typical Atomic-Consistency-Isolation-Durability<br />\n(ACID) type consistency is found in the relational<br />\ndatabases. It implies that there can be short intervals of inconsistency<br />\namong the replicated nodes during which the data gets updated among<br />\nthese nodes. In other words, the replicas are updated asynchronously.</p>\n<h2>Cassandra&rsquo;s high-level architecture</h2>\n<p>Cassandra runs on a peer-to-peer architecture which<br />\nmeans that all nodes in the cluster have equal responsibilities except<br />\nthat some of them are seed nodes for other non-seed nodes to obtain<br />\ninformation about the cluster during startup. Each node holds a<br />\npartition of the database. Cassandra provides automatic data<br />\ndistribution and replication across all nodes in the cluster. Parameters<br />\nare provided to customize the distribution and replication behaviors.<br />\nOnce configured, these operations are processed in the background and<br />\nare fully transparent to the application developers.</p>\n<p>Cassandra is a column-family store and provides great schemaless<br />\nflexibility to application developers. It is designed to manage huge<br />\nvolume of data in a large cluster without a single point of failure. As<br />\nmultiple copies of the same data (replicas) are replicated in the<br />\ncluster, whenever one node fails for whatever reason, the other replicas<br />\nare still available. Replication can be configured to meet the different<br />\nphysical cluster settings, including data center and rack locations.</p>\n<p>Any node in the cluster can accept read or write requests from a client.<br />\nThe node that is connected to a client with a request serves as the<br />\ncoordinator of that particular request. The coordinator determines which<br />\nnodes are responsible for holding the data for the request and acts as a<br />\nproxy between the client and the nodes.</p>\n<p>Cassandra borrows the commitlog mechanism from Google BigTable to ensure<br />\ndata durability. Whenever a write data request is received by a node, it<br />\nis written into the commitlog. The data that is being updated is then<br />\nwritten to a memory structure, known as memtable. When the memtable is<br />\nfull, the data inside the memtable is flushed to a disk storage<br />\nstructure, SSTable. The writes are automatically partitioned by the row<br />\nkey and replicated to the other nodes holding the same partition.</p>\n<p>Cassandra provides linear scalability, which means that the performance<br />\nand capacity of the cluster is proportional to the number of nodes in<br />\nit.</p>\n<h3>Partitioning</h3>\n<p>The ability to scale horizontally and incrementally<br />\nis a Cassandra key design feature. To achieve this, Cassandra is required to dynamically partition<br />\nthe data over the set of nodes in the cluster.</p>\n<p>A cluster is the outermost structure which is<br />\ncomposed of nodes in Cassandra. It is also a container of keyspace. A<br />\nkeyspace in Cassandra is analogous to a schema in a<br />\nrelational database. Each Cassandra cluster has a system keyspace to<br />\nkeep system-wide metadata. It contains the replication settings which<br />\ncontrols how the data is distributed and replicated in a cluster.<br />\nTypically, one keyspace is assigned to one cluster but one cluster might<br />\ncontain more than one keyspace.</p>\n<p>The smallest cluster in the theory contains a single node and a cluster<br />\nof three or more nodes, which is much more practical. Each node holds a<br />\nreplica for the different range of data in partitions, and exchanges<br />\ninformation across the cluster every second.</p>\n<p>A client issues read or write requests to any node. The node that<br />\nreceives the request becomes a coordinator that acts as a proxy of the<br />\nclient to do the things as explained previously. Data is distributed<br />\nacross the cluster and the node addressing mechanism is called<br />\nconsistent hashing. Therefore, a cluster can be viewed as a ring of hash<br />\nas each node in the cluster or the ring is assigned a single unique<br />\ntoken so that each node is responsible for the data in the range from<br />\nits assigned token to that of the previous node. For example, in the<br />\nfollowing figure, a cluster contains four nodes with unique tokens:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_02.jpg\" alt=\"\" /></p>\n<p>:::<br />\nCassandra&rsquo;s consistent hashing<br />\n:::</p>\n<p>Before Version 1.2, tokens were calculated and<br />\nassigned manually and from Version 1.2 onwards,<br />\ntokens can be generated automatically. Each row has a row key used by a<br />\npartitioner to calculate its hash value. The hash<br />\nvalue determines the node which stores the first replica of the row. The<br />\npartitioner is just a hash function that is used for calculating a row<br />\nkey&rsquo;s hash value and it also affects how the data is distributed or<br />\nbalanced in the cluster. When a write occurs, the first replica of the<br />\nrow is always placed in the node with the key range of the token. For<br />\nexample, the hash value of a row key <code>ORACLE</code> is<br />\n<code>6DE7</code> that falls in the range of 4,000 and 8,000 and so the<br />\nrow goes to the bottom node first. All the remaining replicas are<br />\ndistributed based on the replication strategy.</p>\n<h3>Note</h3>\n<p><strong>Consistent hashing</strong></p>\n<p>Consistent hashing allows each node in the cluster<br />\nto independently determine which nodes are replicas for a given row key.<br />\nIt just involves hashing the row key, and then compares that hash value<br />\nto the token of each node in the cluster. If the hash value falls in<br />\nbetween a node&rsquo;s token, and the token of the previous node in the ring<br />\n(tokens are assigned to nodes in a clockwise direction), that node is<br />\nthe replica for that row.<br />\n:::</p>\n<h3>Replication</h3>\n<p>Cassandra uses replication to attain high availability and data durability. Each data is replicated at<br />\na number of nodes that are configured by a parameter called replication<br />\nfactor. The coordinator commands the replication of the data within its<br />\nrange. It replicates the data to the other nodes in the ring. Cassandra<br />\nprovides the client with various configurable options to see how the<br />\ndata is to be replicated, which is called replication strategy.</p>\n<p>Replication strategy is the method of determining which nodes the<br />\nreplicas are placed in. It provides many options, such as rack-aware,<br />\nrack-unaware, network-topology-aware, so on and so forth.</p>\n<h3>Snitch</h3>\n<p>A snitch determines which data centers and racks to<br />\ngo for in order to make Cassandra aware of the<br />\nnetwork topology for routing the  requests<br />\nefficiently. It affects how the replicas can be distributed while<br />\nconsidering the physical setting of the data centers and racks. The node<br />\nlocation can be determined by the rack and data center with reference to<br />\nthe node&rsquo;s IP address. An example of a cluster across two data centers<br />\nis shown in the following figure, in order to illustrate the<br />\nrelationship among replication factor, replication strategy, and snitch<br />\nin a better way:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_03.jpg\" alt=\"\" /></p>\n<p>:::<br />\nMultiple data center cluster<br />\n:::</p>\n<p>Each data center has two racks and each rack contains two nodes<br />\nrespectively. The replication factor per data center is set to three<br />\nhere. With two data centers, there are six replicas in total. The node<br />\nlocation that addresses the data center and rack locations are subject<br />\nto the convention of IP address assignment of the nodes.</p>\n<h3>Seed node</h3>\n<p>Some nodes in a Cassandra cluster are designated as seed nodes for the others. They are<br />\nconfigured to be the first nodes to start in the cluster. They also<br />\nfacilitate the bootstrapping process for the new nodes joining the<br />\ncluster. When a new node comes online, it will talk to the seed node to<br />\nobtain information about the other nodes in the cluster. The talking<br />\nmechanism is called  <strong>gossip</strong>. If a<br />\ncluster is across multiple data centers, the best practice is to have<br />\nmore than one seed node per data center.</p>\n<h3>Gossip and Failure detection</h3>\n<p>Nodes need to communicate periodically (every<br />\nsecond) to exchange state information (for example,<br />\ndead or alive), about themselves and about other nodes they know about.<br />\nCassandra uses a gossip communication protocol to disseminate the state<br />\ninformation, which is  also known as epidemic protocol. It is a peer-to-peer communication<br />\nprotocol that provides a decentralized, periodic,<br />\nand an automatic way for the nodes in the cluster to exchange the state<br />\ninformation about themselves, and about other nodes they know about with<br />\nup to three other nodes. Therefore, all nodes can quickly learn about<br />\nall the other nodes in the cluster. Gossip information is also persisted<br />\nlocally by each node to allow fast restart.</p>\n<p>Cassandra uses a very efficient algorithm, called <em>Phi Accrual Failure<br />\nDetection Algorithm</em>, to detect the<br />\nfailure of a node. The idea of the algorithm is that the failure<br />\ndetection is not represented by a Boolean value stating whether a node<br />\nis up or down. Instead, the algorithm outputs a value on the continuous<br />\nsuspicion level between dead and alive, on how confident it is that the<br />\nnode has failed. In a distributed environment, false negatives might<br />\nhappen due to the network performance, fluctuating workload, and other<br />\nconditions. The algorithm takes all these factors into account and<br />\nprovides a probabilistic value. If a node has failed, the other nodes<br />\nperiodically try to gossip with it to see if it comes back online. A<br />\nnode can then determine locally from the gossip state and its history<br />\nand adjust routes accordingly.</p>\n<h3>Write path</h3>\n<p>The following figure depicts the  components and<br />\ntheir sequence of executions that form a write path:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_04.jpg\" alt=\"\" /></p>\n<p>:::<br />\nCassandra write path<br />\n:::</p>\n<p>When a write occurs, the data will be immediately appended to the<br />\ncommitlog on the disk to ensure write durability. Then Cassandra stores<br />\nthe data in memtable, an in-memory store of hot and fresh data. When<br />\nmemtable is full, the memtable data will be flushed to a disk file,<br />\ncalled SSTable, using sequential I/O and so random I/O is avoided. This<br />\nis the reason why the write performance is so high. The commitlog is<br />\npurged after the flush.</p>\n<p>Due to the intentional adoption of sequential I/O, a row is typically<br />\nstored across many SSTable files. Apart from its data, SSTable also has<br />\na primary index and a <em>bloom filter</em>. A primary index is a<br />\nlist of row keys and the start position of rows in the data file.</p>\n<h3>Note</h3>\n<p><strong>Bloom filter</strong></p>\n<p>Bloom filter is a sample subset of the primary index<br />\nwith very fast nondeterministic algorithms to check whether an element<br />\nis a member of a set. It is used to boost the performance.<br />\n:::</p>\n<p>For write operations, Cassandra supports tunable<br />\nconsistency by various write consistency levels. The write consistency<br />\nlevel is the number of replicas that acknowledge a successful write. It<br />\nis tunable on a spectrum of write consistency levels, as shown in the<br />\nfollowing figure:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_05.jpg\" alt=\"\" /></p>\n<p>:::<br />\nCassandra write consistency levels<br />\n:::</p>\n<p>The following describes the terms in the figure:</p>\n<ul>\n<li>\n<p><strong>ANY</strong>: This is the lowest<br />\nconsistency (but highest availability)</p>\n</li>\n<li>\n<p><strong>ALL</strong>: This is  the highest<br />\nconsistency (but lowest availability)</p>\n</li>\n<li>\n<p><strong>ONE</strong>: This gives at least one<br />\nreplica</p>\n</li>\n<li>\n<p><strong>TWO</strong>: This gives at least two<br />\nreplicas</p>\n</li>\n<li>\n<p><strong>THREE</strong>: This gives at least three<br />\nreplicas</p>\n</li>\n<li>\n<p><strong>QUORUM</strong>: This ensures strong<br />\nconsistency by tolerating some level of failure, which is determined<br />\nby <em>(replication_factor / 2) + 1</em> (rounded down to the<br />\nnearest integer)</p>\n</li>\n<li>\n<p><strong>LOCAL_QUORUM</strong>: This is  for<br />\nmulti-data center and rack-aware without inter-data center traffic</p>\n</li>\n<li>\n<p><strong>EACH_QUORUM</strong>: This is for<br />\nmulti-data center and rack-aware<br />\n:::</p>\n</li>\n</ul>\n<p>The two extremes are the leftmost <strong>ANY</strong><br />\nwhich means weak consistency and the rightmost <strong>ALL</strong> means<br />\nstrong consistency. A consistency level of<br />\n<strong>THREE</strong> is very common in practice. <strong>QUORUM</strong><br />\ncan be chosen to be an optimum value, as calculated by the given<br />\nformula. Here, the replication factor is the number of replicas of data<br />\non multiple nodes. Both <strong>LOCAL QUORUM</strong> and <strong>EACH<br />\nQUORUM</strong> support multiple data centers and rack-aware write<br />\nconsistency with a slight difference as shown earlier.</p>\n<h3>Read path</h3>\n<p>On the flip side, the following figure shows the<br />\ncomponents and their sequence of executions that<br />\nform a read path:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_06.jpg\" alt=\"\" /></p>\n<p>:::<br />\nCassandra read path<br />\n:::</p>\n<p>When a read request comes in to a node, the data to<br />\nbe returned is merged from all the related SSTables and any unflushed<br />\nmemtables. Timestamps are used to determine which one is up-to-date. The<br />\nmerged value is also stored in a write-through row cache to improve the<br />\nfuture read performance.</p>\n<p>Similar to the write consistency levels, Cassandra also provides tunable<br />\nread consistency levels, as shown in the following figure:</p>\n<p><img src=\"https://raw.githubusercontent.com/fenago/apache-cassandra/master/images/8884OS_01_07.jpg\" alt=\"\" /></p>\n<p>:::<br />\nCassandra read consistency levels<br />\n:::</p>\n<p>The following describes the terms in the figure:</p>\n<ul>\n<li>\n<p><strong>ALL</strong>: This is  the highest<br />\nconsistency (but lowest availability)</p>\n</li>\n<li>\n<p><strong>ONE</strong>: This gives at least one<br />\nreplica</p>\n</li>\n<li>\n<p><strong>TWO</strong>: This gives at least two<br />\nreplicas</p>\n</li>\n<li>\n<p><strong>THREE</strong>: This  gives at least three<br />\nreplicas</p>\n</li>\n<li>\n<p><strong>QUORUM</strong>: This ensures strong<br />\nconsistency by tolerating some level of failure, which is determined<br />\nby <em>(replication_factor / 2) + 1</em> (rounded down to the<br />\nnearest integer)</p>\n</li>\n<li>\n<p><strong>LOCAL_QUORUM</strong>: This is  for<br />\nmulti-data center and rack-aware without inter-data center traffic</p>\n</li>\n<li>\n<p><strong>EACH_QUORUM</strong>: This is for<br />\nmulti-data center and rack-aware<br />\n:::</p>\n</li>\n</ul>\n<p>Read consistency level is the number of replicas contacted for a<br />\nsuccessful, consistent read, almost identical to write consistency<br />\nlevels, except that <strong>ANY</strong> is not an option here.</p>\n<h3>Repair mechanism</h3>\n<p>There are three built-in repair mechanisms provided<br />\nby Cassandra:</p>\n<ul>\n<li>\n<p>Read repair</p>\n</li>\n<li>\n<p>Hinted handoff</p>\n</li>\n<li>\n<p>Anti-entropy node repair<br />\n:::</p>\n</li>\n</ul>\n<p>During a read, the coordinator that is just the node connects and<br />\nservices the client, contacts a number of nodes as specified by the<br />\nconsistency level for data and the fastest replicas will return the data<br />\nfor a consistency check by in-memory comparison. As it is not a<br />\ndedicated node, Cassandra lacks a single point of failure. It also<br />\nchecks all the remaining replicas in the background. If a replica is<br />\nfound to be inconsistent, the coordinator will issue an update to bring<br />\nback the consistency. This mechanism is called<br />\n<strong>read repair</strong>.</p>\n<p><strong>Hinted handoff</strong> aims at reducing the<br />\ntime to restore a failed node when rejoining the cluster. It ensures<br />\nabsolute write availability by sacrificing a bit of<br />\nread consistency. If a replica is down at the time a write occurs,<br />\nanother healthy replica stores a hint. Even worse, if all the relevant<br />\nreplicas are down, the coordinator stores the hint locally. The hint<br />\nbasically contains the location of the failed replica, the affected row<br />\nkey, and the actual data that is being written. When a node responsible<br />\nfor the token range is up again, the hint will be handed off to resume<br />\nthe write. As such, the update cannot be read before a complete handoff,<br />\nleading to inconsistent reads.</p>\n<p>Another repair mechanism is called<br />\n<strong>anti-entropy</strong> which is a replica synchronization mechanism<br />\nto ensure up-to-date data on all nodes and is run by the administrators<br />\nmanually.</p>\n<h2>Features of Cassandra</h2>\n<p>In order to keep this lab short, the following<br />\nbullet list covers the great features provided by Cassandra:</p>\n<ul>\n<li>\n<p>Written in Java and hence providing native Java support</p>\n</li>\n<li>\n<p>Blend of Google BigTable and Amazon Dynamo</p>\n</li>\n<li>\n<p>Flexible schemaless column-family data model</p>\n</li>\n<li>\n<p>Support for structured and unstructured data</p>\n</li>\n<li>\n<p>Decentralized, distributed peer-to-peer architecture</p>\n</li>\n<li>\n<p>Multi-data center and rack-aware data replication</p>\n</li>\n<li>\n<p>Location transparent</p>\n</li>\n<li>\n<p>Cloud enabled</p>\n</li>\n<li>\n<p>Fault-tolerant with no single point of failure</p>\n</li>\n<li>\n<p>An automatic and transparent failover</p>\n</li>\n<li>\n<p>Elastic, massively, and linearly scalable</p>\n</li>\n<li>\n<p>Online node addition or removal</p>\n</li>\n<li>\n<p>High Performance</p>\n</li>\n<li>\n<p>Built-in data compression</p>\n</li>\n<li>\n<p>Built-in caching layer</p>\n</li>\n<li>\n<p>Write-optimized</p>\n</li>\n<li>\n<p>Tunable consistency providing choices from very strong consistency<br />\nto different levels of eventual consistency</p>\n</li>\n<li>\n<p>Provision of <strong>Cassandra Query Language</strong><br />\n(<strong>CQL</strong>), a SQL-like language<br />\nimitating <code>INSERT</code> , <code>UPDATE</code> ,<br />\n<code>DELETE</code> , <code>SELECT</code> syntax of SQL</p>\n</li>\n<li>\n<p>Open source and community-driven</p>\n</li>\n</ul>\n<h2>Summary</h2>\n<p>In this lab, we have gone through a bit of history starting from the<br />\n1970s. We were in total control of the data models that were rather<br />\nstable and the applications that were pretty simple. The relational<br />\ndatabases were a perfect fit in the old days. With the emergence of<br />\nobject-oriented programming and the explosion of the web applications on<br />\nthe pervasive Internet, the nature of the data has been extended from<br />\nstructured to semi-structured and unstructured. Also, the application<br />\nhas become more complex. The relational databases could not be perfect<br />\nagain. The concept of Big Data was created to describe such challenges<br />\nand NoSQL databases provide an alternative resolution to the relational<br />\ndatabases.</p>\n<p>NoSQL databases are of a wide variety. They provide some common benefits<br />\nand can be classified by the NoSQL database type. Apache Cassandra is<br />\none of the NoSQL databases that is a blend of Google BigTable and Amazon<br />\nDynamo. The elegance of its architecture inherits from the DNA of these<br />\ntwo parents.</p>\n<p>In the next lab, we will look at the flexible data model supported<br />\nby Cassandra.</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1591515272522_-86775418",
      "id": "paragraph_1591515272522_-86775418",
      "dateCreated": "2020-06-07T07:34:32+0000",
      "dateStarted": "2020-06-07T07:42:44+0000",
      "dateFinished": "2020-06-07T07:42:44+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:4971"
    },
    {
      "user": "anonymous",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1591515298112_1820638193",
      "id": "paragraph_1591515298112_1820638193",
      "dateCreated": "2020-06-07T07:34:58+0000",
      "status": "READY",
      "$$hashKey": "object:4972"
    }
  ],
  "name": "lab_1",
  "id": "2FCXJKMEX",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview1",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/lab_1"
}